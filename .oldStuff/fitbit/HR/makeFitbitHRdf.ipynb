{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chowder/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeWorkingHRDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"fitbit/17-9-24/takeout-20240917T195619Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 16052785 rows from 43 files\n"
     ]
    }
   ],
   "source": [
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-14\n"
     ]
    }
   ],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "else:\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 32915 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 50 seconds to make 16 million rows\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def to_iso(s):\n",
    "    return '20' + s[6:8] + '-' + s[0:2] + '-' + s[3:5] + s[8:]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        sampleDT = datetime.fromisoformat(to_iso(row[\"dateTime\"]))\n",
    "        samplesList.append([sampleDT, \n",
    "                            row[\"value\"][\"confidence\"], \n",
    "                            row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 1_000_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_localize('UTC')\n",
    "fitbitHRdf.index = pd.to_datetime(fitbitHRdf.index)\n",
    "\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashes match for 2020-05-13T100219-0700_2020-06-15T175149-0700_9b3f3630_.parquet.gzip\n",
      "hashes match for 2020-06-15T175149-0700_2020-07-19T103931-0700_15ed86cc_.parquet.gzip\n",
      "hashes match for 2020-07-19T103931-0700_2020-08-22T011641-0700_0c4966ee_.parquet.gzip\n",
      "hashes match for 2020-08-22T011641-0700_2020-09-25T101037-0700_efba1909_.parquet.gzip\n",
      "hashes match for 2020-09-25T101037-0700_2020-10-28T182707-0700_5f1b0c2b_.parquet.gzip\n",
      "hashes match for 2020-10-28T182707-0700_2020-12-01T022613-0700_ad3308cd_.parquet.gzip\n",
      "hashes match for 2020-12-01T022613-0700_2021-01-04T062220-0700_1bb757ba_.parquet.gzip\n",
      "hashes match for 2021-01-04T062220-0700_2021-02-05T204607-0700_691ba674_.parquet.gzip\n",
      "hashes match for 2021-02-05T204607-0700_2021-03-10T204558-0700_033c3397_.parquet.gzip\n",
      "hashes match for 2021-03-10T204558-0700_2021-04-12T181209-0700_8473fbe8_.parquet.gzip\n",
      "hashes match for 2021-04-12T181209-0700_2021-05-15T211855-0700_75a7df30_.parquet.gzip\n",
      "hashes match for 2021-05-15T211855-0700_2021-06-17T102224-0700_b74adca4_.parquet.gzip\n",
      "hashes match for 2021-06-17T102224-0700_2021-07-20T155538-0700_3b7441f5_.parquet.gzip\n",
      "hashes match for 2021-07-20T155538-0700_2021-08-22T211108-0700_845bb2c2_.parquet.gzip\n",
      "hashes match for 2021-08-22T211108-0700_2021-09-25T095705-0700_dc680060_.parquet.gzip\n",
      "hashes match for 2021-09-25T095705-0700_2021-10-29T180139-0700_967326a0_.parquet.gzip\n",
      "hashes match for 2021-10-29T180139-0700_2021-12-07T041321-0700_6305c057_.parquet.gzip\n",
      "hashes match for 2021-12-07T041321-0700_2022-01-17T151845-0700_5328ec38_.parquet.gzip\n",
      "hashes match for 2022-01-17T151845-0700_2022-02-23T231305-0700_f9480063_.parquet.gzip\n",
      "hashes match for 2022-02-23T231305-0700_2022-04-04T000613-0700_50d5b891_.parquet.gzip\n",
      "hashes match for 2022-04-04T000613-0700_2022-05-08T231123-0700_949cb47d_.parquet.gzip\n",
      "hashes match for 2022-05-08T231123-0700_2022-06-12T193752-0700_01be1b0c_.parquet.gzip\n",
      "hashes match for 2022-06-12T193752-0700_2022-07-20T152353-0700_87491e5b_.parquet.gzip\n",
      "hashes match for 2022-07-20T152353-0700_2022-09-04T055733-0700_1f138b52_.parquet.gzip\n",
      "hashes match for 2022-09-04T055733-0700_2022-10-07T184245-0700_b6e013e8_.parquet.gzip\n",
      "hashes match for 2022-10-07T184245-0700_2022-11-16T183545-0700_6fd3ef7f_.parquet.gzip\n",
      "hashes match for 2022-11-16T183545-0700_2022-12-22T162543-0700_bc0826b4_.parquet.gzip\n",
      "hashes match for 2022-12-22T162543-0700_2023-01-22T232300-0700_47343c4b_.parquet.gzip\n",
      "hashes match for 2023-01-22T232300-0700_2023-03-03T180831-0700_a57b1413_.parquet.gzip\n",
      "hashes match for 2023-03-03T180831-0700_2023-04-10T111549-0700_94aae0dc_.parquet.gzip\n",
      "hashes match for 2023-04-10T111549-0700_2023-05-29T024606-0700_1258e91a_.parquet.gzip\n",
      "hashes match for 2023-05-29T024606-0700_2023-07-03T095828-0700_54c946c8_.parquet.gzip\n",
      "hashes match for 2023-07-03T095828-0700_2023-08-08T062534-0700_c0b327d9_.parquet.gzip\n",
      "hashes match for 2023-08-08T062534-0700_2023-09-12T133818-0700_5670ad48_.parquet.gzip\n",
      "hashes match for 2023-09-12T133818-0700_2023-11-20T221525-0700_f6de8548_.parquet.gzip\n",
      "hashes match for 2023-11-20T221525-0700_2023-12-23T112306-0700_2f9c16c1_.parquet.gzip\n",
      "hashes match for 2023-12-23T112306-0700_2024-01-28T041118-0700_f0f39539_.parquet.gzip\n",
      "hashes match for 2024-01-28T041118-0700_2024-03-03T214546-0700_e8ec58b5_.parquet.gzip\n",
      "hashes match for 2024-03-03T214546-0700_2024-04-08T191904-0700_84cae969_.parquet.gzip\n",
      "hashes match for 2024-04-08T191904-0700_2024-05-08T134233-0700_4fd6a3c4_.parquet.gzip\n",
      "hashes match for 2024-05-08T134233-0700_2024-06-07T222822-0700_5356b0de_.parquet.gzip\n",
      "hashes match for 2024-06-07T222822-0700_2024-07-07T203643-0700_426265b9_.parquet.gzip\n",
      "hashes match for 2024-07-07T203643-0700_2024-09-17T125220-0700_e949c8e5_.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7379ceed\n",
      "7379ceed test\n",
      "7379ceed 2020-05-13T100219-0700\n",
      "2020-05-13T100219-0700 7379ceed\n"
     ]
    }
   ],
   "source": [
    "print(short_hash(fitbitHRdf))\n",
    "print(short_hash(fitbitHRdf) + \" \" + \"test\")\n",
    "print(short_hash(fitbitHRdf) + \" \" + fitbitHRdf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z'))\n",
    "print(fitbitHRdf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') + \" \" + short_hash(fitbitHRdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5b98ce5\n",
      "d5b98ce5 test\n",
      "d5b98ce5 2020-05-13T100219-0700\n",
      "2020-05-13T100219-0700 30d49dfb\n",
      "30d49dfb 2020-05-13T100219-0700\n",
      "30d49dfb test\n",
      "confidence     0\n",
      "value         70\n",
      "Name: 2020-05-13 10:02:19-07:00, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ilocdf = fitbitHRdf.iloc[0:100_000]\n",
    "# print(ilocdf.iloc[0])\n",
    "print(short_hash(ilocdf))\n",
    "print(short_hash(ilocdf) + \" \" + \"test\")\n",
    "print(short_hash(ilocdf) + \" \" + ilocdf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z'))\n",
    "\n",
    "#once iloc is called once \n",
    "print(ilocdf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') + \" \" + short_hash(ilocdf))\n",
    "print(short_hash(ilocdf) + \" \" + ilocdf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z'))\n",
    "print(short_hash(ilocdf) + \" \" + \"test\")\n",
    "print(ilocdf.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2e90c5d\n",
      "Name                         John\n",
      "Age                            23\n",
      "City                     New York\n",
      "Salary                      50000\n",
      "Department                     HR\n",
      "Timestamp     2024-01-01 00:00:00\n",
      "Name: 0, dtype: object\n",
      "59b6270c\n",
      "59b6270c\n",
      "c2e90c5d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import pickle\n",
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]\n",
    "\n",
    "# Define the data with a timestamp column\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Eve', 'Charlie', 'David', 'Frank', 'Grace', 'Hannah', 'Ian'],\n",
    "    'Age': [23, 25, 30, 22, 35, 29, 40, 27, 31, 28],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose'],\n",
    "    'Salary': [50000, 60000, 70000, 45000, 52000, 58000, 62000, 54000, 56000, 59000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'HR', 'Marketing', 'IT', 'Finance', 'Marketing', 'HR', 'Finance'],\n",
    "    'Timestamp': pd.date_range(start='2024-01-01', periods=10, freq='D')  # 10 sequential dates\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "testdf = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "slicedf = testdf.iloc[0:5]\n",
    "\n",
    "print(short_hash(slicedf))\n",
    "pi = slicedf\n",
    "pic = slicedf.copy()\n",
    "print(str(slicedf.iloc[0]))\n",
    "print(short_hash(slicedf))\n",
    "print(short_hash(pi))\n",
    "print(short_hash(pic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f6189ca7\n",
      "Name                         John\n",
      "Age                            23\n",
      "City                     New York\n",
      "Salary                      50000\n",
      "Department                     HR\n",
      "Timestamp     2024-01-01 00:00:00\n",
      "Name: 0, dtype: object\n",
      "f6189ca7\n",
      "f6189ca7\n",
      "f6189ca7\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame\n",
    "testdf = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "slicedf = testdf.iloc[0:5]\n",
    "\n",
    "print(short_hash(slicedf.values))\n",
    "pi = slicedf\n",
    "pic = slicedf.copy()\n",
    "print(str(slicedf.iloc[0]))\n",
    "print(short_hash(slicedf.values))\n",
    "print(short_hash(pi.values))\n",
    "print(short_hash(pic.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
