{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#if possible every update function need a way to skip the files that have already been integrated\n",
    "# for the export and the read in, although this might be a bit of an optimization\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heart_rate-2025-01-05.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exportDataPath = \"/home/chowder/Documents/dataExports/fitbit/25-1-7/takeout-20250106T144047Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportDataPath)\n",
    "hrFilenames = sorted([x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"])\n",
    "hrFilenames[-1]\n",
    "# no reading yet we can test that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 1000000 samples so far\n",
      "added 2000000 samples so far\n",
      "added 3000000 samples so far\n",
      "added 4000000 samples so far\n",
      "added 5000000 samples so far\n",
      "added 6000000 samples so far\n",
      "added 7000000 samples so far\n",
      "added 8000000 samples so far\n",
      "added 9000000 samples so far\n",
      "added 10000000 samples so far\n",
      "added 11000000 samples so far\n",
      "added 12000000 samples so far\n",
      "added 13000000 samples so far\n",
      "added 14000000 samples so far\n",
      "added 15000000 samples so far\n",
      "added 16000000 samples so far\n",
      "added 17000000 samples so far\n",
      "added 17335490 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 50 seconds to make 17 million rows\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def to_iso(s):\n",
    "    return '20' + s[6:8] + '-' + s[0:2] + '-' + s[3:5] + s[8:]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportDataPath + fn))\n",
    "    for row in data:\n",
    "        sampleDT = datetime.fromisoformat(to_iso(row[\"dateTime\"]))\n",
    "        samplesList.append([sampleDT, \n",
    "                            row[\"value\"][\"confidence\"], \n",
    "                            row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 1_000_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chowder/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_localize('UTC')\n",
    "fitbitHRdf.index = pd.to_datetime(fitbitHRdf.index)\n",
    "\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's redo the writeWorkingTSDf\n",
    "#ngl I like the short hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this writes a file for a timeSeries of a DF\n",
    "def writeTimeSeriesDf(TSDf, targetPath):\n",
    "    sh = short_hash(TSDf)\n",
    "    parquetName = TSDf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" +\\\n",
    "                TSDf.iloc[-1].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" + sh + \"_\" + \".parquet.gzip\"\n",
    "    print(f\"saved to a file named {parquetName}\")\n",
    "\n",
    "    TSDf.to_parquet(targetPath + parquetName,\n",
    "            compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a dataframe you want to save and does it in multiple files\n",
    "#with the number of rows between rows_per_file and 2 * rows_per_file \n",
    "def saveRows(TSDf, targetPath, rows_per_file):\n",
    "    if len(TSDf) == 0: return\n",
    "    startRow = 0\n",
    "    endRow = len(TSDf)\n",
    "    rows_remaining = endRow - startRow\n",
    "    while rows_remaining > 2 * rows_per_file:\n",
    "        print(f'{rows_remaining} is too many rows writing {startRow} to {(endRow - rows_remaining) + rows_per_file}')\n",
    "        writeTimeSeriesDf(TSDf.iloc[startRow: (endRow - rows_remaining) + rows_per_file + 1], targetPath)\n",
    "        rows_remaining -= rows_per_file\n",
    "        startRow += rows_per_file\n",
    "    writeTimeSeriesDf(TSDf.iloc[startRow:endRow+1], targetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a df returns the approximate number of rows to get a target file size \n",
    "def calcRowsPerFile(Df, targetFileSize, targetPath, fileName = 'test.parquet.gzip'):\n",
    "    if fileName == 'test.parquet.gzip':\n",
    "        fileRows = 1_000_000\n",
    "        if len(Df) < fileRows: fileRows = len(Df)-1\n",
    "        Df.iloc[:fileRows].to_parquet(targetPath + fileName,\n",
    "                        compression='gzip')\n",
    "        file_size = os.path.getsize(targetPath + fileName)\n",
    "        os.remove(targetPath + fileName)\n",
    "    else:\n",
    "        fileRows = len(pd.read_parquet(targetPath + fileName))\n",
    "        file_size = os.path.getsize(targetPath + fileName)\n",
    "    \n",
    "    rows_per_file = int(fileRows//(file_size/targetFileSize))\n",
    "    return rows_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates over exiting files and adds rows in the relevant scope\n",
    "def writeToExistingTSFiles(TSDf, fileNames, targetPath, rows_per_file):\n",
    "    tzi = TSDf.index[0].tzinfo\n",
    "    for fileNum, fileName in enumerate(fileNames):\n",
    "        if fileNum == 0:\n",
    "            startTime = TSDf.index[0]\n",
    "        else:\n",
    "            startTime = pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        if len(fileNames) == 1 or fileNum == len(fileNames) - 1:\n",
    "            endTime = TSDf.index[-1]\n",
    "        else:\n",
    "            endTime = pd.to_datetime(fileNames[fileNum + 1].split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        # if the hash doesn't match write a new file\n",
    "        if short_hash(TSDf.loc[startTime:endTime]) != fileName.split('_')[2]:\n",
    "            print(\"the hashes don't match\")\n",
    "            os.remove(targetPath + fileName)\n",
    "            saveRows(TSDf.loc[startTime:endTime], targetPath, rows_per_file)\n",
    "        else:\n",
    "            print(f'hashes match for {fileName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this may be the only function I need to change\n",
    "# the features I want to add is to \n",
    "# take in partyName, deviceName, dataType, dataSource\n",
    "# convert the DF to UTC\n",
    "\n",
    "#TODO: rename the wordkingData file OldWorkingData\n",
    "#I should make the update/impot function be the one that handles diffrent sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeWorkingTSDf(partyName, deviceName, dataType, dataSource, TSDf, targetFileSize = 2 * 1024 * 1024):\n",
    "    dataFolderName = \"_\".join([partyName, deviceName, dataType, dataSource]) + \"/\"\n",
    "    fullPath = workingDataPath + dataFolderName\n",
    "    if not os.path.exists(fullPath):\n",
    "        os.makedirs(fullPath)\n",
    "    currentFileNames = sorted(os.listdir(fullPath))\n",
    "\n",
    "    TSDf.index = TSDf.index.tz_convert('UTC')\n",
    "\n",
    "\n",
    "    if len(currentFileNames) == 0:\n",
    "        rows_per_file = calcRowsPerFile(TSDf, targetFileSize, fullPath)\n",
    "        saveRows(TSDf, fullPath, rows_per_file)\n",
    "\n",
    "    else:\n",
    "        rows_per_file = calcRowsPerFile(TSDf, targetFileSize, fullPath, currentFileNames[0])\n",
    "        writeToExistingTSFiles(TSDf, currentFileNames, fullPath, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17335490 is too many rows writing 0 to 365451\n",
      "saved to a file named 2020-05-13T170219+0000_2020-06-16T005314+0000_100a0466_.parquet.gzip\n",
      "16970039 is too many rows writing 365451 to 730902\n",
      "saved to a file named 2020-06-16T005314+0000_2020-07-19T174401+0000_4818a063_.parquet.gzip\n",
      "16604588 is too many rows writing 730902 to 1096353\n",
      "saved to a file named 2020-07-19T174401+0000_2020-08-22T082256+0000_4464f29d_.parquet.gzip\n",
      "16239137 is too many rows writing 1096353 to 1461804\n",
      "saved to a file named 2020-08-22T082256+0000_2020-09-25T171817+0000_573f9dcc_.parquet.gzip\n",
      "15873686 is too many rows writing 1461804 to 1827255\n",
      "saved to a file named 2020-09-25T171817+0000_2020-10-29T013912+0000_bd11fc15_.parquet.gzip\n",
      "15508235 is too many rows writing 1827255 to 2192706\n",
      "saved to a file named 2020-10-29T013912+0000_2020-12-01T093854+0000_91dcda5b_.parquet.gzip\n",
      "15142784 is too many rows writing 2192706 to 2558157\n",
      "saved to a file named 2020-12-01T093854+0000_2021-01-04T133306+0000_992b3849_.parquet.gzip\n",
      "14777333 is too many rows writing 2558157 to 2923608\n",
      "saved to a file named 2021-01-04T133306+0000_2021-02-06T040057+0000_4bc7455d_.parquet.gzip\n",
      "14411882 is too many rows writing 2923608 to 3289059\n",
      "saved to a file named 2021-02-06T040057+0000_2021-03-11T040427+0000_82789e58_.parquet.gzip\n",
      "14046431 is too many rows writing 3289059 to 3654510\n",
      "saved to a file named 2021-03-11T040427+0000_2021-04-13T013126+0000_2b49363b_.parquet.gzip\n",
      "13680980 is too many rows writing 3654510 to 4019961\n",
      "saved to a file named 2021-04-13T013126+0000_2021-05-16T043816+0000_c2fb5816_.parquet.gzip\n",
      "13315529 is too many rows writing 4019961 to 4385412\n",
      "saved to a file named 2021-05-16T043816+0000_2021-06-17T174317+0000_07a4c167_.parquet.gzip\n",
      "12950078 is too many rows writing 4385412 to 4750863\n",
      "saved to a file named 2021-06-17T174317+0000_2021-07-20T232151+0000_f8055ecc_.parquet.gzip\n",
      "12584627 is too many rows writing 4750863 to 5116314\n",
      "saved to a file named 2021-07-20T232151+0000_2021-08-23T044013+0000_707f97f2_.parquet.gzip\n",
      "12219176 is too many rows writing 5116314 to 5481765\n",
      "saved to a file named 2021-08-23T044013+0000_2021-09-25T172751+0000_5fde37a7_.parquet.gzip\n",
      "11853725 is too many rows writing 5481765 to 5847216\n",
      "saved to a file named 2021-09-25T172751+0000_2021-10-30T013714+0000_b2042d2c_.parquet.gzip\n",
      "11488274 is too many rows writing 5847216 to 6212667\n",
      "saved to a file named 2021-10-30T013714+0000_2021-12-07T114819+0000_c502e208_.parquet.gzip\n",
      "11122823 is too many rows writing 6212667 to 6578118\n",
      "saved to a file named 2021-12-07T114819+0000_2022-01-17T230636+0000_a7a650ac_.parquet.gzip\n",
      "10757372 is too many rows writing 6578118 to 6943569\n",
      "saved to a file named 2022-01-17T230636+0000_2022-02-24T064741+0000_daec901a_.parquet.gzip\n",
      "10391921 is too many rows writing 6943569 to 7309020\n",
      "saved to a file named 2022-02-24T064741+0000_2022-04-04T075520+0000_463a3c04_.parquet.gzip\n",
      "10026470 is too many rows writing 7309020 to 7674471\n",
      "saved to a file named 2022-04-04T075520+0000_2022-05-09T065102+0000_1d519113_.parquet.gzip\n",
      "9661019 is too many rows writing 7674471 to 8039922\n",
      "saved to a file named 2022-05-09T065102+0000_2022-06-13T032017+0000_4e9b0432_.parquet.gzip\n",
      "9295568 is too many rows writing 8039922 to 8405373\n",
      "saved to a file named 2022-06-13T032017+0000_2022-07-20T230352+0000_561a272e_.parquet.gzip\n",
      "8930117 is too many rows writing 8405373 to 8770824\n",
      "saved to a file named 2022-07-20T230352+0000_2022-09-04T134749+0000_83d66f0a_.parquet.gzip\n",
      "8564666 is too many rows writing 8770824 to 9136275\n",
      "saved to a file named 2022-09-04T134749+0000_2022-10-08T023923+0000_061045e8_.parquet.gzip\n",
      "8199215 is too many rows writing 9136275 to 9501726\n",
      "saved to a file named 2022-10-08T023923+0000_2022-11-17T022334+0000_a4e50c31_.parquet.gzip\n",
      "7833764 is too many rows writing 9501726 to 9867177\n",
      "saved to a file named 2022-11-17T022334+0000_2022-12-22T234521+0000_df73ef8d_.parquet.gzip\n",
      "7468313 is too many rows writing 9867177 to 10232628\n",
      "saved to a file named 2022-12-22T234521+0000_2023-01-23T073219+0000_83eaddad_.parquet.gzip\n",
      "7102862 is too many rows writing 10232628 to 10598079\n",
      "saved to a file named 2023-01-23T073219+0000_2023-03-04T021133+0000_5b056e41_.parquet.gzip\n",
      "6737411 is too many rows writing 10598079 to 10963530\n",
      "saved to a file named 2023-03-04T021133+0000_2023-04-10T191415+0000_7e637fd7_.parquet.gzip\n",
      "6371960 is too many rows writing 10963530 to 11328981\n",
      "saved to a file named 2023-04-10T191415+0000_2023-05-29T104644+0000_daa192d7_.parquet.gzip\n",
      "6006509 is too many rows writing 11328981 to 11694432\n",
      "saved to a file named 2023-05-29T104644+0000_2023-07-03T180559+0000_73c1b236_.parquet.gzip\n",
      "5641058 is too many rows writing 11694432 to 12059883\n",
      "saved to a file named 2023-07-03T180559+0000_2023-08-08T142809+0000_67bb5f09_.parquet.gzip\n",
      "5275607 is too many rows writing 12059883 to 12425334\n",
      "saved to a file named 2023-08-08T142809+0000_2023-09-12T213915+0000_952bdbfd_.parquet.gzip\n",
      "4910156 is too many rows writing 12425334 to 12790785\n",
      "saved to a file named 2023-09-12T213915+0000_2023-11-21T062331+0000_085f77a4_.parquet.gzip\n",
      "4544705 is too many rows writing 12790785 to 13156236\n",
      "saved to a file named 2023-11-21T062331+0000_2023-12-23T192318+0000_811e58c4_.parquet.gzip\n",
      "4179254 is too many rows writing 13156236 to 13521687\n",
      "saved to a file named 2023-12-23T192318+0000_2024-01-28T122204+0000_330d0dd0_.parquet.gzip\n",
      "3813803 is too many rows writing 13521687 to 13887138\n",
      "saved to a file named 2024-01-28T122204+0000_2024-03-04T060122+0000_17a96cd6_.parquet.gzip\n",
      "3448352 is too many rows writing 13887138 to 14252589\n",
      "saved to a file named 2024-03-04T060122+0000_2024-04-09T033512+0000_4101c242_.parquet.gzip\n",
      "3082901 is too many rows writing 14252589 to 14618040\n",
      "saved to a file named 2024-04-09T033512+0000_2024-05-08T220359+0000_5f6a7f8f_.parquet.gzip\n",
      "2717450 is too many rows writing 14618040 to 14983491\n",
      "saved to a file named 2024-05-08T220359+0000_2024-06-08T063937+0000_d3a4dce6_.parquet.gzip\n",
      "2351999 is too many rows writing 14983491 to 15348942\n",
      "saved to a file named 2024-06-08T063937+0000_2024-07-08T050300+0000_ed87257c_.parquet.gzip\n",
      "1986548 is too many rows writing 15348942 to 15714393\n",
      "saved to a file named 2024-07-08T050300+0000_2024-08-16T132243+0000_dc64c419_.parquet.gzip\n",
      "1621097 is too many rows writing 15714393 to 16079844\n",
      "saved to a file named 2024-08-16T132243+0000_2024-09-19T231523+0000_45ceff30_.parquet.gzip\n",
      "1255646 is too many rows writing 16079844 to 16445295\n",
      "saved to a file named 2024-09-19T231523+0000_2024-10-18T012435+0000_137d3d80_.parquet.gzip\n",
      "890195 is too many rows writing 16445295 to 16810746\n",
      "saved to a file named 2024-10-18T012435+0000_2024-11-19T221903+0000_202d8f2b_.parquet.gzip\n",
      "saved to a file named 2024-11-19T221903+0000_2025-01-06T065957+0000_b8b22ae4_.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "workingDataPath = \"/home/chowder/Documents/workingData/\"\n",
    "writeWorkingTSDf(\"fitbit\", \"charge4or5\", \"hr\", \"builtin\", fitbitHRdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wooo the write  looks like it's working!\n",
    "# now I'll do the writes for apple and polar\n",
    "# none of these dataframes are so big that we can't just\n",
    "# read it in, add to it, sort it, deduplicate it, write it back for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "# Function to compute a short hash of a Python object\n",
    "def short_hash(obj, length=8):\n",
    "    # Serialize the object using pickle\n",
    "    obj_bytes = pickle.dumps(obj)\n",
    "    \n",
    "    # Compute MD5 hash of the serialized object\n",
    "    hash_obj = hashlib.md5(obj_bytes)\n",
    "    \n",
    "    # Return the hash truncated to the specified length\n",
    "    return hash_obj.hexdigest()[:length]\n",
    "\n",
    "# this writes a file for a subset of a DF\n",
    "def writeHRDfFile(HRDf, workingDataHRPath):\n",
    "    sh = short_hash(HRDf)\n",
    "    parquetName = HRDf.iloc[0].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" +\\\n",
    "                HRDf.iloc[-1].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                \"_\" + sh + \"_\" + \".parquet.gzip\"\n",
    "    print(f\"saved to a file named {parquetName}\")\n",
    "\n",
    "    HRDf.to_parquet(workingDataHRPath + parquetName,\n",
    "            compression='gzip') \n",
    "\n",
    "#takes in a dataframe you want to save and does it in multiple files\n",
    "def saveRows(df, workingDataHRPath, rows_per_file):\n",
    "    if len(df) == 0: return\n",
    "    startRow = 0\n",
    "    endRow = len(df)\n",
    "    rows_remaining = endRow - startRow\n",
    "    while rows_remaining > 2 * rows_per_file:\n",
    "        print(f'{rows_remaining} is too many rows writing {startRow} to {(endRow - rows_remaining) + rows_per_file}')\n",
    "        writeHRDfFile(df.iloc[startRow: (endRow - rows_remaining) + rows_per_file + 1], workingDataHRPath)\n",
    "        rows_remaining -= rows_per_file\n",
    "        startRow += rows_per_file\n",
    "    writeHRDfFile(df.iloc[startRow:endRow+1], workingDataHRPath)\n",
    "\n",
    "def rowsPerFile(Df, targetFileSize, workingDataHRPath, fileName = 'test.parquet.gzip'):\n",
    "    if fileName == 'test.parquet.gzip':\n",
    "        fileRows = 1_000_000\n",
    "        if len(Df) < fileRows: fileRows = len(Df)-1\n",
    "        Df.iloc[:fileRows].to_parquet(workingDataHRPath + fileName,\n",
    "                        compression='gzip')\n",
    "        file_size = os.path.getsize(workingDataHRPath + fileName)\n",
    "        os.remove(workingDataHRPath + fileName)\n",
    "    else:\n",
    "        fileRows = len(pd.read_parquet(workingDataHRPath + fileName))\n",
    "        file_size = os.path.getsize(workingDataHRPath + fileName)\n",
    "    \n",
    "    rows_per_file = int(fileRows//(file_size/targetFileSize))\n",
    "    return rows_per_file\n",
    "\n",
    "def writeToExistingHRFiles(HRDf, fileNames, workingDataHRPath, rows_per_file):\n",
    "    tzi = HRDf.index[0].tzinfo\n",
    "    for fileNum, fileName in enumerate(fileNames):\n",
    "        if fileNum == 0:\n",
    "            startTime = HRDf.index[0]\n",
    "        else:\n",
    "            startTime = pd.to_datetime(fileName.split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        if len(fileNames) == 1 or fileNum == len(fileNames) - 1:\n",
    "            endTime = HRDf.index[-1]\n",
    "        else:\n",
    "            endTime = pd.to_datetime(fileNames[fileNum + 1].split('_')[0]).tz_convert(tzi)\n",
    "        \n",
    "        # if the hash doesn't match write a new file\n",
    "        if short_hash(HRDf.loc[startTime:endTime]) != fileName.split('_')[2]:\n",
    "            print(\"the hashes don't match\")\n",
    "            os.remove(workingDataHRPath + fileName)\n",
    "            saveRows(HRDf.loc[startTime:endTime], workingDataHRPath, rows_per_file)\n",
    "        else:\n",
    "            print(f'hashes match for {fileName}')\n",
    "\n",
    "def writeWorkingHRDfParquet(deviceName, HRDf, clearFiles = True, targetFileSize = 2 * 1024 * 1024):\n",
    "    workingDataHRPath = workingDataPath + deviceName + \"/hr/\"\n",
    "    fileNames = sorted(os.listdir(workingDataHRPath))\n",
    "\n",
    "    if len(fileNames) == 0:\n",
    "        rows_per_file = calcRowsPerFile(HRDf, targetFileSize, workingDataHRPath)\n",
    "        saveRows(HRDf, workingDataHRPath, rows_per_file)\n",
    "\n",
    "    else:\n",
    "        rows_per_file = calcRowsPerFile(HRDf, targetFileSize, workingDataHRPath, fileNames[0])\n",
    "        writeToExistingHRFiles(HRDf, fileNames, workingDataHRPath, rows_per_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
