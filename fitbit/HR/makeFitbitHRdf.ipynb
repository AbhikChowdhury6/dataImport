{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chowder/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeWorkingHRDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"fitbit/17-9-24/takeout-20240917T195619Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 0 rows from 0 files\n"
     ]
    }
   ],
   "source": [
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "else:\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 1000000 samples so far\n",
      "added 2000000 samples so far\n",
      "added 3000000 samples so far\n",
      "added 4000000 samples so far\n",
      "added 5000000 samples so far\n",
      "added 6000000 samples so far\n",
      "added 7000000 samples so far\n",
      "added 8000000 samples so far\n",
      "added 9000000 samples so far\n",
      "added 10000000 samples so far\n",
      "added 11000000 samples so far\n",
      "added 12000000 samples so far\n",
      "added 13000000 samples so far\n",
      "added 14000000 samples so far\n",
      "added 15000000 samples so far\n",
      "added 16000000 samples so far\n",
      "added 16052785 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 50 seconds to make 16 million rows\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "def to_iso(s):\n",
    "    return '20' + s[6:8] + '-' + s[0:2] + '-' + s[3:5] + s[8:]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        sampleDT = datetime.fromisoformat(to_iso(row[\"dateTime\"]))\n",
    "        samplesList.append([sampleDT, \n",
    "                            row[\"value\"][\"confidence\"], \n",
    "                            row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 1_000_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_localize('UTC')\n",
    "\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_convert(\"US/Arizona\")\n",
    "fitbitHRdf.index = pd.to_datetime(fitbitHRdf.index)\n",
    "\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16052785 is too many rows writing 0 to 365436\n",
      "saved to a file named 2020-05-13T100219-0700_2020-06-15T175149-0700_9b3f3630_.parquet.gzip\n",
      "15687349 is too many rows writing 365436 to 730872\n",
      "saved to a file named 2020-06-15T175149-0700_2020-07-19T103931-0700_15ed86cc_.parquet.gzip\n",
      "15321913 is too many rows writing 730872 to 1096308\n",
      "saved to a file named 2020-07-19T103931-0700_2020-08-22T011641-0700_0c4966ee_.parquet.gzip\n",
      "14956477 is too many rows writing 1096308 to 1461744\n",
      "saved to a file named 2020-08-22T011641-0700_2020-09-25T101037-0700_efba1909_.parquet.gzip\n",
      "14591041 is too many rows writing 1461744 to 1827180\n",
      "saved to a file named 2020-09-25T101037-0700_2020-10-28T182707-0700_5f1b0c2b_.parquet.gzip\n",
      "14225605 is too many rows writing 1827180 to 2192616\n",
      "saved to a file named 2020-10-28T182707-0700_2020-12-01T022613-0700_ad3308cd_.parquet.gzip\n",
      "13860169 is too many rows writing 2192616 to 2558052\n",
      "saved to a file named 2020-12-01T022613-0700_2021-01-04T062220-0700_1bb757ba_.parquet.gzip\n",
      "13494733 is too many rows writing 2558052 to 2923488\n",
      "saved to a file named 2021-01-04T062220-0700_2021-02-05T204607-0700_691ba674_.parquet.gzip\n",
      "13129297 is too many rows writing 2923488 to 3288924\n",
      "saved to a file named 2021-02-05T204607-0700_2021-03-10T204558-0700_033c3397_.parquet.gzip\n",
      "12763861 is too many rows writing 3288924 to 3654360\n",
      "saved to a file named 2021-03-10T204558-0700_2021-04-12T181209-0700_8473fbe8_.parquet.gzip\n",
      "12398425 is too many rows writing 3654360 to 4019796\n",
      "saved to a file named 2021-04-12T181209-0700_2021-05-15T211855-0700_75a7df30_.parquet.gzip\n",
      "12032989 is too many rows writing 4019796 to 4385232\n",
      "saved to a file named 2021-05-15T211855-0700_2021-06-17T102224-0700_b74adca4_.parquet.gzip\n",
      "11667553 is too many rows writing 4385232 to 4750668\n",
      "saved to a file named 2021-06-17T102224-0700_2021-07-20T155538-0700_3b7441f5_.parquet.gzip\n",
      "11302117 is too many rows writing 4750668 to 5116104\n",
      "saved to a file named 2021-07-20T155538-0700_2021-08-22T211108-0700_845bb2c2_.parquet.gzip\n",
      "10936681 is too many rows writing 5116104 to 5481540\n",
      "saved to a file named 2021-08-22T211108-0700_2021-09-25T095705-0700_dc680060_.parquet.gzip\n",
      "10571245 is too many rows writing 5481540 to 5846976\n",
      "saved to a file named 2021-09-25T095705-0700_2021-10-29T180139-0700_967326a0_.parquet.gzip\n",
      "10205809 is too many rows writing 5846976 to 6212412\n",
      "saved to a file named 2021-10-29T180139-0700_2021-12-07T041321-0700_6305c057_.parquet.gzip\n",
      "9840373 is too many rows writing 6212412 to 6577848\n",
      "saved to a file named 2021-12-07T041321-0700_2022-01-17T151845-0700_5328ec38_.parquet.gzip\n",
      "9474937 is too many rows writing 6577848 to 6943284\n",
      "saved to a file named 2022-01-17T151845-0700_2022-02-23T231305-0700_f9480063_.parquet.gzip\n",
      "9109501 is too many rows writing 6943284 to 7308720\n",
      "saved to a file named 2022-02-23T231305-0700_2022-04-04T000613-0700_50d5b891_.parquet.gzip\n",
      "8744065 is too many rows writing 7308720 to 7674156\n",
      "saved to a file named 2022-04-04T000613-0700_2022-05-08T231123-0700_949cb47d_.parquet.gzip\n",
      "8378629 is too many rows writing 7674156 to 8039592\n",
      "saved to a file named 2022-05-08T231123-0700_2022-06-12T193752-0700_01be1b0c_.parquet.gzip\n",
      "8013193 is too many rows writing 8039592 to 8405028\n",
      "saved to a file named 2022-06-12T193752-0700_2022-07-20T152353-0700_87491e5b_.parquet.gzip\n",
      "7647757 is too many rows writing 8405028 to 8770464\n",
      "saved to a file named 2022-07-20T152353-0700_2022-09-04T055733-0700_1f138b52_.parquet.gzip\n",
      "7282321 is too many rows writing 8770464 to 9135900\n",
      "saved to a file named 2022-09-04T055733-0700_2022-10-07T184245-0700_b6e013e8_.parquet.gzip\n",
      "6916885 is too many rows writing 9135900 to 9501336\n",
      "saved to a file named 2022-10-07T184245-0700_2022-11-16T183545-0700_6fd3ef7f_.parquet.gzip\n",
      "6551449 is too many rows writing 9501336 to 9866772\n",
      "saved to a file named 2022-11-16T183545-0700_2022-12-22T162543-0700_bc0826b4_.parquet.gzip\n",
      "6186013 is too many rows writing 9866772 to 10232208\n",
      "saved to a file named 2022-12-22T162543-0700_2023-01-22T232300-0700_47343c4b_.parquet.gzip\n",
      "5820577 is too many rows writing 10232208 to 10597644\n",
      "saved to a file named 2023-01-22T232300-0700_2023-03-03T180831-0700_a57b1413_.parquet.gzip\n",
      "5455141 is too many rows writing 10597644 to 10963080\n",
      "saved to a file named 2023-03-03T180831-0700_2023-04-10T111549-0700_94aae0dc_.parquet.gzip\n",
      "5089705 is too many rows writing 10963080 to 11328516\n",
      "saved to a file named 2023-04-10T111549-0700_2023-05-29T024606-0700_1258e91a_.parquet.gzip\n",
      "4724269 is too many rows writing 11328516 to 11693952\n",
      "saved to a file named 2023-05-29T024606-0700_2023-07-03T095828-0700_54c946c8_.parquet.gzip\n",
      "4358833 is too many rows writing 11693952 to 12059388\n",
      "saved to a file named 2023-07-03T095828-0700_2023-08-08T062534-0700_c0b327d9_.parquet.gzip\n",
      "3993397 is too many rows writing 12059388 to 12424824\n",
      "saved to a file named 2023-08-08T062534-0700_2023-09-12T133818-0700_5670ad48_.parquet.gzip\n",
      "3627961 is too many rows writing 12424824 to 12790260\n",
      "saved to a file named 2023-09-12T133818-0700_2023-11-20T221525-0700_f6de8548_.parquet.gzip\n",
      "3262525 is too many rows writing 12790260 to 13155696\n",
      "saved to a file named 2023-11-20T221525-0700_2023-12-23T112306-0700_2f9c16c1_.parquet.gzip\n",
      "2897089 is too many rows writing 13155696 to 13521132\n",
      "saved to a file named 2023-12-23T112306-0700_2024-01-28T041118-0700_f0f39539_.parquet.gzip\n",
      "2531653 is too many rows writing 13521132 to 13886568\n",
      "saved to a file named 2024-01-28T041118-0700_2024-03-03T214546-0700_e8ec58b5_.parquet.gzip\n",
      "2166217 is too many rows writing 13886568 to 14252004\n",
      "saved to a file named 2024-03-03T214546-0700_2024-04-08T191904-0700_84cae969_.parquet.gzip\n",
      "1800781 is too many rows writing 14252004 to 14617440\n",
      "saved to a file named 2024-04-08T191904-0700_2024-05-08T134233-0700_4fd6a3c4_.parquet.gzip\n",
      "1435345 is too many rows writing 14617440 to 14982876\n",
      "saved to a file named 2024-05-08T134233-0700_2024-06-07T222822-0700_5356b0de_.parquet.gzip\n",
      "1069909 is too many rows writing 14982876 to 15348312\n",
      "saved to a file named 2024-06-07T222822-0700_2024-07-07T203643-0700_426265b9_.parquet.gzip\n",
      "saved to a file named 2024-07-07T203643-0700_2024-09-17T125220-0700_e949c8e5_.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
