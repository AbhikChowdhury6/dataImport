{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the list of all files and directories\n",
    "workingDataPath = \"/home/chowder/Documents/workingData/fitbit/hr/\"\n",
    "\n",
    "pathOfExport = \"/home/chowder/Documents/dataExports/fitbit/\"\n",
    "exportFilePath = \"27-8-24/takeout-20240828T033834Z-001/Takeout/Fitbit/Global Export Data\"\n",
    "\n",
    "dir_list = os.listdir(pathOfExport + exportFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 15826297 number of rows from 19 files\n"
     ]
    }
   ],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(workingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")\n",
    "\n",
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} number of rows from {len(workingDataFiles)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 8, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "datesSoFar[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heart_rate-2023-02-05.json',\n",
       " 'heart_rate-2023-11-14.json',\n",
       " 'heart_rate-2024-08-26.json',\n",
       " 'heart_rate-2023-09-13.json',\n",
       " 'heart_rate-2022-10-12.json',\n",
       " 'heart_rate-2023-04-13.json',\n",
       " 'heart_rate-2024-08-27.json',\n",
       " 'heart_rate-2024-02-07.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]\n",
    "hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "hrFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 34326 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 40 minutes to make 15.8 million rows\n",
    "import json\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(pathOfExport + exportFilePath + \"/\" + fn))\n",
    "    for row in data:\n",
    "        samplesList.append([pd.to_datetime(row[\"dateTime\"] + \"-0700\"), row[\"value\"][\"confidence\"], row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 100_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the file in bytes\n",
    "workingDataCompleteFile = \"/home/chowder/Documents/workingData/fitbit/hr/fitbitHRdf.parquet.gzip\"\n",
    "fitbitHRdf.to_parquet(workingDataCompleteFile,\n",
    "              compression='gzip') \n",
    "\n",
    "file_size = os.path.getsize(workingDataCompleteFile)\n",
    "os.remove(workingDataCompleteFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file size of all the data is about 86 MB\n",
      "the total number of rows in the file is 15826297\n",
      "splitting into 18 number of files with 879238 rows per file\n",
      "\n",
      "saving rows 0 to 879237\n",
      "to a file named 2020-05-13 17:02:34-07:00_2020-08-02 07:27:56-07:00.parquet.gzip\n",
      "saving rows 879238 to 1758475\n",
      "to a file named 2020-08-02 07:28:51-07:00_2020-10-22 19:11:11-07:00.parquet.gzip\n",
      "saving rows 1758476 to 2637713\n",
      "to a file named 2020-10-22 19:12:06-07:00_2021-01-11 15:28:18-07:00.parquet.gzip\n",
      "saving rows 2637714 to 3516951\n",
      "to a file named 2021-01-11 15:28:33-07:00_2021-03-31 17:02:38-07:00.parquet.gzip\n",
      "saving rows 3516952 to 4396189\n",
      "to a file named 2021-03-31 17:03:13-07:00_2021-06-18 16:06:46-07:00.parquet.gzip\n",
      "saving rows 4396190 to 5275427\n",
      "to a file named 2021-06-18 16:07:22-07:00_2021-09-06 21:51:09-07:00.parquet.gzip\n",
      "saving rows 5275428 to 6154665\n",
      "to a file named 2021-09-06 21:51:44-07:00_2021-11-30 20:39:36-07:00.parquet.gzip\n",
      "saving rows 6154666 to 7033903\n",
      "to a file named 2021-11-30 20:40:06-07:00_2022-03-04 16:25:48-07:00.parquet.gzip\n",
      "saving rows 7033904 to 7913141\n",
      "to a file named 2022-03-04 16:26:03-07:00_2022-06-01 09:28:15-07:00.parquet.gzip\n",
      "saving rows 7913142 to 8792379\n",
      "to a file named 2022-06-01 09:28:45-07:00_2022-09-06 15:41:09-07:00.parquet.gzip\n",
      "saving rows 8792380 to 9671617\n",
      "to a file named 2022-09-06 15:41:39-07:00_2022-12-02 21:25:00-07:00.parquet.gzip\n",
      "saving rows 9671618 to 10550855\n",
      "to a file named 2022-12-02 21:25:12-07:00_2023-02-27 11:01:14-07:00.parquet.gzip\n",
      "saving rows 10550856 to 11430093\n",
      "to a file named 2023-02-27 11:01:49-07:00_2023-06-08 21:38:00-07:00.parquet.gzip\n",
      "saving rows 11430094 to 12309331\n",
      "to a file named 2023-06-08 21:38:25-07:00_2023-09-01 20:10:22-07:00.parquet.gzip\n",
      "saving rows 12309332 to 13188569\n",
      "to a file named 2023-09-01 20:10:37-07:00_2023-12-26 16:25:51-07:00.parquet.gzip\n",
      "saving rows 13188570 to 14067807\n",
      "to a file named 2023-12-26 16:26:31-07:00_2024-03-21 19:19:22-07:00.parquet.gzip\n",
      "saving rows 14067808 to 14947045\n",
      "to a file named 2024-03-21 19:20:02-07:00_2024-06-05 02:49:42-07:00.parquet.gzip\n",
      "saving rows 14947046 to 15826283\n",
      "to a file named 2024-06-05 02:50:02-07:00_2024-08-28 06:58:15-07:00.parquet.gzip\n",
      "saving rows 15826284 to 15826296\n",
      "to a file named 2024-08-28 06:58:20-07:00_2024-08-28 06:59:50-07:00.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "numFiles = math.ceil(file_size / (1024 * 1024 * 5))\n",
    "rows_per_file = int(len(fitbitHRdf)/numFiles)\n",
    "\n",
    "print(f\"the file size of all the data is about {file_size // (1024 * 1024)} MB\")\n",
    "print(f\"the total number of rows in the file is {len(fitbitHRdf)}\")\n",
    "print(f\"splitting into {numFiles} number of files with {rows_per_file} rows per file\")\n",
    "print()\n",
    "\n",
    "for fileNumber in range(numFiles + 1):\n",
    "    startRow = fileNumber * rows_per_file\n",
    "    if fileNumber == numFiles:\n",
    "        endRow = len(fitbitHRdf) - 1\n",
    "    else:\n",
    "        endRow = ((fileNumber + 1) * rows_per_file) - 1\n",
    "\n",
    "    print(f\"saving rows {startRow} to {endRow}\")\n",
    "    \n",
    "    parquetName = str(fitbitHRdf.iloc[startRow].name) +\\\n",
    "                  \"_\" +\\\n",
    "                  str(fitbitHRdf.iloc[endRow].name) +\\\n",
    "                  \".parquet.gzip\"\n",
    "    print(f\"to a file named {parquetName}\")\n",
    "\n",
    "    fitbitHRdf.iloc[startRow:endRow+1].to_parquet(workingDataPath + parquetName,\n",
    "              compression='gzip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
