{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeWorkingHRDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"fitbit/27-8-24/takeout-20240828T033834Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 15826367 rows from 19 files\n"
     ]
    }
   ],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")\n",
    "\n",
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-24\n"
     ]
    }
   ],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "else:\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 37927 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 40 minutes to make 15.8 million rows\n",
    "import json\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        samplesList.append([pd.to_datetime(row[\"dateTime\"] + \"+0000\").tz_convert(\"US/Arizona\"), row[\"value\"][\"confidence\"], row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 100_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file size of all the data is about 86 MB\n",
      "the total number of rows in the file is 15826367\n",
      "splitting into 18 number of about 5MB files with 879242 rows per file\n",
      "saving rows 0 to 879241\n",
      "confidence     0\n",
      "value         70\n",
      "Name: 2020-05-13 10:02:19-07:00, dtype: object\n",
      "to a file named 2020-05-13T100219-0700_2020-08-02T002831-0700.parquet.gzip\n",
      "2020-05-13 10:02:19-07:00\n",
      "saving rows 879242 to 1758483\n",
      "confidence     3\n",
      "value         67\n",
      "Name: 2020-08-02 00:28:36-07:00, dtype: object\n",
      "to a file named 2020-08-02T002836-0700_2020-10-22T121146-0700.parquet.gzip\n",
      "2020-08-02 00:28:36-07:00\n",
      "saving rows 1758484 to 2637725\n",
      "confidence     1\n",
      "value         86\n",
      "Name: 2020-10-22 12:12:01-07:00, dtype: object\n",
      "to a file named 2020-10-22T121201-0700_2021-01-11T082828-0700.parquet.gzip\n",
      "2020-10-22 12:12:01-07:00\n",
      "saving rows 2637726 to 3516967\n",
      "confidence     3\n",
      "value         60\n",
      "Name: 2021-01-11 08:28:33-07:00, dtype: object\n",
      "to a file named 2021-01-11T082833-0700_2021-03-31T100308-0700.parquet.gzip\n",
      "2021-01-11 08:28:33-07:00\n",
      "saving rows 3516968 to 4396209\n",
      "confidence     2\n",
      "value         81\n",
      "Name: 2021-03-31 10:03:13-07:00, dtype: object\n",
      "to a file named 2021-03-31T100313-0700_2021-06-18T090701-0700.parquet.gzip\n",
      "2021-03-31 10:03:13-07:00\n",
      "saving rows 4396210 to 5275451\n",
      "confidence     2\n",
      "value         89\n",
      "Name: 2021-06-18 09:07:07-07:00, dtype: object\n",
      "to a file named 2021-06-18T090707-0700_2021-09-06T145124-0700.parquet.gzip\n",
      "2021-06-18 09:07:07-07:00\n",
      "saving rows 5275452 to 6154693\n",
      "confidence     2\n",
      "value         88\n",
      "Name: 2021-09-06 14:51:34-07:00, dtype: object\n",
      "to a file named 2021-09-06T145134-0700_2021-11-30T133951-0700.parquet.gzip\n",
      "2021-09-06 14:51:34-07:00\n",
      "saving rows 6154694 to 7033935\n",
      "confidence     1\n",
      "value         98\n",
      "Name: 2021-11-30 13:39:56-07:00, dtype: object\n",
      "to a file named 2021-11-30T133956-0700_2022-03-04T092553-0700.parquet.gzip\n",
      "2021-11-30 13:39:56-07:00\n",
      "saving rows 7033936 to 7913177\n",
      "confidence     3\n",
      "value         67\n",
      "Name: 2022-03-04 09:25:58-07:00, dtype: object\n",
      "to a file named 2022-03-04T092558-0700_2022-06-01T022820-0700.parquet.gzip\n",
      "2022-03-04 09:25:58-07:00\n",
      "saving rows 7913178 to 8792419\n",
      "confidence     3\n",
      "value         57\n",
      "Name: 2022-06-01 02:28:25-07:00, dtype: object\n",
      "to a file named 2022-06-01T022825-0700_2022-09-06T084114-0700.parquet.gzip\n",
      "2022-06-01 02:28:25-07:00\n",
      "saving rows 8792420 to 9671661\n",
      "confidence     3\n",
      "value         61\n",
      "Name: 2022-09-06 08:41:29-07:00, dtype: object\n",
      "to a file named 2022-09-06T084129-0700_2022-12-02T142503-0700.parquet.gzip\n",
      "2022-09-06 08:41:29-07:00\n",
      "saving rows 9671662 to 10550903\n",
      "confidence     2\n",
      "value         76\n",
      "Name: 2022-12-02 14:25:06-07:00, dtype: object\n",
      "to a file named 2022-12-02T142506-0700_2023-02-27T040124-0700.parquet.gzip\n",
      "2022-12-02 14:25:06-07:00\n",
      "saving rows 10550904 to 11430145\n",
      "confidence     3\n",
      "value         59\n",
      "Name: 2023-02-27 04:01:29-07:00, dtype: object\n",
      "to a file named 2023-02-27T040129-0700_2023-06-08T143815-0700.parquet.gzip\n",
      "2023-02-27 04:01:29-07:00\n",
      "saving rows 11430146 to 12309387\n",
      "confidence     2\n",
      "value         72\n",
      "Name: 2023-06-08 14:38:25-07:00, dtype: object\n",
      "to a file named 2023-06-08T143825-0700_2023-09-01T131032-0700.parquet.gzip\n",
      "2023-06-08 14:38:25-07:00\n",
      "saving rows 12309388 to 13188629\n",
      "confidence     2\n",
      "value         86\n",
      "Name: 2023-09-01 13:10:37-07:00, dtype: object\n",
      "to a file named 2023-09-01T131037-0700_2023-12-26T092601-0700.parquet.gzip\n",
      "2023-09-01 13:10:37-07:00\n",
      "saving rows 13188630 to 14067871\n",
      "confidence     1\n",
      "value         83\n",
      "Name: 2023-12-26 09:26:11-07:00, dtype: object\n",
      "to a file named 2023-12-26T092611-0700_2024-03-21T121937-0700.parquet.gzip\n",
      "2023-12-26 09:26:11-07:00\n",
      "saving rows 14067872 to 14947113\n",
      "confidence     2\n",
      "value         74\n",
      "Name: 2024-03-21 12:19:52-07:00, dtype: object\n",
      "to a file named 2024-03-21T121952-0700_2024-06-04T194947-0700.parquet.gzip\n",
      "2024-03-21 12:19:52-07:00\n",
      "saving rows 14947114 to 15826355\n",
      "confidence     2\n",
      "value         69\n",
      "Name: 2024-06-04 19:49:52-07:00, dtype: object\n",
      "to a file named 2024-06-04T194952-0700_2024-08-27T235825-0700.parquet.gzip\n",
      "2024-06-04 19:49:52-07:00\n",
      "saving rows 15826356 to 15826366\n",
      "confidence     3\n",
      "value         56\n",
      "Name: 2024-08-27 23:58:40-07:00, dtype: object\n",
      "to a file named 2024-08-27T235840-0700_2024-08-27T235950-0700.parquet.gzip\n",
      "2024-08-27 23:58:40-07:00\n"
     ]
    }
   ],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the size of the file in bytes\n",
    "# workingDataCompleteFile = \"/home/chowder/Documents/workingData/fitbit/hr/fitbitHRdf.parquet.gzip\"\n",
    "# fitbitHRdf.to_parquet(workingDataCompleteFile,\n",
    "#               compression='gzip') \n",
    "\n",
    "# file_size = os.path.getsize(workingDataCompleteFile)\n",
    "# os.remove(workingDataCompleteFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# numFiles = math.ceil(file_size / (1024 * 1024 * 5))\n",
    "# rows_per_file = int(len(fitbitHRdf)/numFiles)\n",
    "\n",
    "# print(f\"the file size of all the data is about {file_size // (1024 * 1024)} MB\")\n",
    "# print(f\"the total number of rows in the file is {len(fitbitHRdf)}\")\n",
    "# print(f\"splitting into {numFiles} number of files with {rows_per_file} rows per file\")\n",
    "\n",
    "# for fileNumber in range(numFiles + 1):\n",
    "#     startRow = fileNumber * rows_per_file\n",
    "#     if fileNumber == numFiles:\n",
    "#         endRow = len(fitbitHRdf) - 1\n",
    "#     else:\n",
    "#         endRow = ((fileNumber + 1) * rows_per_file) - 1\n",
    "\n",
    "#     print(f\"saving rows {startRow} to {endRow}\")\n",
    "#     print(fitbitHRdf.iloc[startRow])\n",
    "\n",
    "#     parquetName = fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \"_\" +\\\n",
    "#                   fitbitHRdf.iloc[endRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \".parquet.gzip\"\n",
    "#     print(f\"to a file named {parquetName}\")\n",
    "\n",
    "#     print(pd.to_datetime(fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z')))\n",
    "\n",
    "#     fitbitHRdf.iloc[startRow:endRow+1].to_parquet(fitbitHRWorkingDataPath + parquetName,\n",
    "#               compression='gzip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
