{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeWorkingHRDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"fitbit/17-9-24/takeout-20240917T195619Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 16052785 rows from 19 files\n"
     ]
    }
   ],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")\n",
    "\n",
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-14\n"
     ]
    }
   ],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "else:\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 32915 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 40 minutes to make 15.8 million rows\n",
    "import json\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        samplesList.append([pd.to_datetime(row[\"dateTime\"] + \"+0000\").tz_convert(\"US/Arizona\"), row[\"value\"][\"confidence\"], row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 100_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file size of all the data is about 87 MB\n",
      "the total number of rows in the file is 16052785\n",
      "splitting into 18 files of about 5MB files with 891821 rows per file\n",
      "saving rows 0 to 891820\n",
      "confidence     0\n",
      "value         70\n",
      "Name: 2020-05-13 10:02:19-07:00, dtype: object\n",
      "to a file named 2020-05-13T100219-0700_2020-08-03T034741-0700.parquet.gzip\n",
      "2020-05-13 10:02:19-07:00\n",
      "saving rows 891821 to 1783641\n",
      "confidence     3\n",
      "value         68\n",
      "Name: 2020-08-03 03:47:46-07:00, dtype: object\n",
      "to a file named 2020-08-03T034746-0700_2020-10-24T175335-0700.parquet.gzip\n",
      "2020-08-03 03:47:46-07:00\n",
      "saving rows 1783642 to 2675462\n",
      "confidence     1\n",
      "value         90\n",
      "Name: 2020-10-24 17:53:50-07:00, dtype: object\n",
      "to a file named 2020-10-24T175350-0700_2021-01-14T183521-0700.parquet.gzip\n",
      "2020-10-24 17:53:50-07:00\n",
      "saving rows 2675463 to 3567283\n",
      "confidence     3\n",
      "value         66\n",
      "Name: 2021-01-14 18:35:31-07:00, dtype: object\n",
      "to a file named 2021-01-14T183531-0700_2021-04-04T220303-0700.parquet.gzip\n",
      "2021-01-14 18:35:31-07:00\n",
      "saving rows 3567284 to 4459104\n",
      "confidence     3\n",
      "value         81\n",
      "Name: 2021-04-04 22:03:08-07:00, dtype: object\n",
      "to a file named 2021-04-04T220308-0700_2021-06-24T003312-0700.parquet.gzip\n",
      "2021-04-04 22:03:08-07:00\n",
      "saving rows 4459105 to 5350925\n",
      "confidence     3\n",
      "value         70\n",
      "Name: 2021-06-24 00:33:17-07:00, dtype: object\n",
      "to a file named 2021-06-24T003317-0700_2021-09-13T131636-0700.parquet.gzip\n",
      "2021-06-24 00:33:17-07:00\n",
      "saving rows 5350926 to 6242746\n",
      "confidence     2\n",
      "value         60\n",
      "Name: 2021-09-13 13:16:46-07:00, dtype: object\n",
      "to a file named 2021-09-13T131646-0700_2021-12-09T235110-0700.parquet.gzip\n",
      "2021-09-13 13:16:46-07:00\n",
      "saving rows 6242747 to 7134567\n",
      "confidence     2\n",
      "value         91\n",
      "Name: 2021-12-09 23:51:15-07:00, dtype: object\n",
      "to a file named 2021-12-09T235115-0700_2022-03-13T154056-0700.parquet.gzip\n",
      "2021-12-09 23:51:15-07:00\n",
      "saving rows 7134568 to 8026388\n",
      "confidence     1\n",
      "value         89\n",
      "Name: 2022-03-13 15:41:11-07:00, dtype: object\n",
      "to a file named 2022-03-13T154111-0700_2022-06-11T141703-0700.parquet.gzip\n",
      "2022-03-13 15:41:11-07:00\n",
      "saving rows 8026389 to 8918209\n",
      "confidence      1\n",
      "value         108\n",
      "Name: 2022-06-11 14:17:08-07:00, dtype: object\n",
      "to a file named 2022-06-11T141708-0700_2022-09-15T223056-0700.parquet.gzip\n",
      "2022-06-11 14:17:08-07:00\n",
      "saving rows 8918210 to 9810030\n",
      "confidence     2\n",
      "value         84\n",
      "Name: 2022-09-15 22:31:01-07:00, dtype: object\n",
      "to a file named 2022-09-15T223101-0700_2022-12-17T094408-0700.parquet.gzip\n",
      "2022-09-15 22:31:01-07:00\n",
      "saving rows 9810031 to 10701851\n",
      "confidence     3\n",
      "value         60\n",
      "Name: 2022-12-17 09:44:13-07:00, dtype: object\n",
      "to a file named 2022-12-17T094413-0700_2023-03-14T014506-0700.parquet.gzip\n",
      "2022-12-17 09:44:13-07:00\n",
      "saving rows 10701852 to 11593672\n",
      "confidence     3\n",
      "value         57\n",
      "Name: 2023-03-14 01:45:11-07:00, dtype: object\n",
      "to a file named 2023-03-14T014511-0700_2023-06-27T174105-0700.parquet.gzip\n",
      "2023-03-14 01:45:11-07:00\n",
      "saving rows 11593673 to 12485493\n",
      "confidence     2\n",
      "value         70\n",
      "Name: 2023-06-27 17:41:08-07:00, dtype: object\n",
      "to a file named 2023-06-27T174108-0700_2023-10-12T024130-0700.parquet.gzip\n",
      "2023-06-27 17:41:08-07:00\n",
      "saving rows 12485494 to 13377314\n",
      "confidence     3\n",
      "value         52\n",
      "Name: 2023-10-12 02:41:35-07:00, dtype: object\n",
      "to a file named 2023-10-12T024135-0700_2024-01-12T193642-0700.parquet.gzip\n",
      "2023-10-12 02:41:35-07:00\n",
      "saving rows 13377315 to 14269135\n",
      "confidence     1\n",
      "value         74\n",
      "Name: 2024-01-12 19:36:57-07:00, dtype: object\n",
      "to a file named 2024-01-12T193657-0700_2024-04-10T073550-0700.parquet.gzip\n",
      "2024-01-12 19:36:57-07:00\n",
      "saving rows 14269136 to 15160956\n",
      "confidence     3\n",
      "value         64\n",
      "Name: 2024-04-10 07:35:55-07:00, dtype: object\n",
      "to a file named 2024-04-10T073555-0700_2024-06-21T013658-0700.parquet.gzip\n",
      "2024-04-10 07:35:55-07:00\n",
      "saving rows 15160957 to 16052777\n",
      "confidence     3\n",
      "value         52\n",
      "Name: 2024-06-21 01:37:03-07:00, dtype: object\n",
      "to a file named 2024-06-21T013703-0700_2024-09-17T125135-0700.parquet.gzip\n",
      "2024-06-21 01:37:03-07:00\n",
      "saving rows 16052778 to 16052784\n",
      "confidence     2\n",
      "value         62\n",
      "Name: 2024-09-17 12:51:45-07:00, dtype: object\n",
      "to a file named 2024-09-17T125145-0700_2024-09-17T125220-0700.parquet.gzip\n",
      "2024-09-17 12:51:45-07:00\n"
     ]
    }
   ],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the size of the file in bytes\n",
    "# workingDataCompleteFile = \"/home/chowder/Documents/workingData/fitbit/hr/fitbitHRdf.parquet.gzip\"\n",
    "# fitbitHRdf.to_parquet(workingDataCompleteFile,\n",
    "#               compression='gzip') \n",
    "\n",
    "# file_size = os.path.getsize(workingDataCompleteFile)\n",
    "# os.remove(workingDataCompleteFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# numFiles = math.ceil(file_size / (1024 * 1024 * 5))\n",
    "# rows_per_file = int(len(fitbitHRdf)/numFiles)\n",
    "\n",
    "# print(f\"the file size of all the data is about {file_size // (1024 * 1024)} MB\")\n",
    "# print(f\"the total number of rows in the file is {len(fitbitHRdf)}\")\n",
    "# print(f\"splitting into {numFiles} number of files with {rows_per_file} rows per file\")\n",
    "\n",
    "# for fileNumber in range(numFiles + 1):\n",
    "#     startRow = fileNumber * rows_per_file\n",
    "#     if fileNumber == numFiles:\n",
    "#         endRow = len(fitbitHRdf) - 1\n",
    "#     else:\n",
    "#         endRow = ((fileNumber + 1) * rows_per_file) - 1\n",
    "\n",
    "#     print(f\"saving rows {startRow} to {endRow}\")\n",
    "#     print(fitbitHRdf.iloc[startRow])\n",
    "\n",
    "#     parquetName = fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \"_\" +\\\n",
    "#                   fitbitHRdf.iloc[endRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \".parquet.gzip\"\n",
    "#     print(f\"to a file named {parquetName}\")\n",
    "\n",
    "#     print(pd.to_datetime(fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z')))\n",
    "\n",
    "#     fitbitHRdf.iloc[startRow:endRow+1].to_parquet(fitbitHRWorkingDataPath + parquetName,\n",
    "#               compression='gzip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
