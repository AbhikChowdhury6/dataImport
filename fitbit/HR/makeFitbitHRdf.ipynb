{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chowder/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeWorkingHRDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"fitbit/17-9-24/takeout-20240917T195619Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataFileName \u001b[38;5;129;01min\u001b[39;00m workingDataFiles:\n\u001b[1;32m      2\u001b[0m     dfSoFar \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([dfSoFar, pd\u001b[38;5;241m.\u001b[39mread_parquet(fitbitHRWorkingDataPath \u001b[38;5;241m+\u001b[39m dataFileName)]) \n\u001b[0;32m----> 4\u001b[0m dfSoFar \u001b[38;5;241m=\u001b[39m dfSoFar[\u001b[38;5;241m~\u001b[39m\u001b[43mdfSoFar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dfSoFar)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(workingDataFiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3020\u001b[0m, in \u001b[0;36mIndex.duplicated\u001b[0;34m(self, keep)\u001b[0m\n\u001b[1;32m   2966\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mduplicated\u001b[39m(\u001b[38;5;28mself\u001b[39m, keep: DropKeep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_]:\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;124;03m    Indicate duplicate index values.\u001b[39;00m\n\u001b[1;32m   2969\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;124;03m    array([ True, False,  True, False,  True])\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3020\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_unique\u001b[49m:\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;66;03m# fastpath available bc we are immutable\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_duplicated(keep\u001b[38;5;241m=\u001b[39mkeep)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:2238\u001b[0m, in \u001b[0;36mIndex.is_unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_unique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;124;03m    Return if the index has unique values.\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_unique\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "else:\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"-\")[0] == \"heart_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 1000000 samples so far\n",
      "added 2000000 samples so far\n",
      "added 3000000 samples so far\n",
      "added 4000000 samples so far\n",
      "added 5000000 samples so far\n",
      "added 6000000 samples so far\n",
      "added 7000000 samples so far\n",
      "added 8000000 samples so far\n",
      "added 9000000 samples so far\n",
      "added 10000000 samples so far\n",
      "added 11000000 samples so far\n",
      "added 12000000 samples so far\n",
      "added 13000000 samples so far\n",
      "added 14000000 samples so far\n",
      "added 15000000 samples so far\n",
      "added 16000000 samples so far\n",
      "added 16052785 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 50 seconds to make 16 million rows\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "def to_iso(s):\n",
    "    return '20' + s[6:8] + '-' + s[0:2] + '-' + s[3:5] + s[8:]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        sampleDT = datetime.fromisoformat(to_iso(row[\"dateTime\"]))\n",
    "        samplesList.append([sampleDT, \n",
    "                            row[\"value\"][\"confidence\"], \n",
    "                            row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 1_000_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_localize('UTC')\n",
    "\n",
    "fitbitHRdf.index = fitbitHRdf.index.tz_convert(\"US/Arizona\")\n",
    "\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file size of all the data is about 87 MB\n",
      "the total number of rows in the file is 16052785\n",
      "splitting into 18 files of about 5MB files with 891821 rows per file\n",
      "saving rows 0 to 891820\n",
      "confidence     0\n",
      "value         70\n",
      "Name: 2020-05-13 10:02:19-07:00, dtype: object\n",
      "to a file named 2020-05-13T100219-0700_2020-08-03T034741-0700.parquet.gzip\n",
      "2020-05-13 10:02:19-07:00\n",
      "saving rows 891821 to 1783641\n",
      "confidence     3\n",
      "value         68\n",
      "Name: 2020-08-03 03:47:46-07:00, dtype: object\n",
      "to a file named 2020-08-03T034746-0700_2020-10-24T175335-0700.parquet.gzip\n",
      "2020-08-03 03:47:46-07:00\n",
      "saving rows 1783642 to 2675462\n",
      "confidence     1\n",
      "value         90\n",
      "Name: 2020-10-24 17:53:50-07:00, dtype: object\n",
      "to a file named 2020-10-24T175350-0700_2021-01-14T183521-0700.parquet.gzip\n",
      "2020-10-24 17:53:50-07:00\n",
      "saving rows 2675463 to 3567283\n",
      "confidence     3\n",
      "value         66\n",
      "Name: 2021-01-14 18:35:31-07:00, dtype: object\n",
      "to a file named 2021-01-14T183531-0700_2021-04-04T220303-0700.parquet.gzip\n",
      "2021-01-14 18:35:31-07:00\n",
      "saving rows 3567284 to 4459104\n",
      "confidence     3\n",
      "value         81\n",
      "Name: 2021-04-04 22:03:08-07:00, dtype: object\n",
      "to a file named 2021-04-04T220308-0700_2021-06-24T003312-0700.parquet.gzip\n",
      "2021-04-04 22:03:08-07:00\n",
      "saving rows 4459105 to 5350925\n",
      "confidence     3\n",
      "value         70\n",
      "Name: 2021-06-24 00:33:17-07:00, dtype: object\n",
      "to a file named 2021-06-24T003317-0700_2021-09-13T131636-0700.parquet.gzip\n",
      "2021-06-24 00:33:17-07:00\n",
      "saving rows 5350926 to 6242746\n",
      "confidence     2\n",
      "value         60\n",
      "Name: 2021-09-13 13:16:46-07:00, dtype: object\n",
      "to a file named 2021-09-13T131646-0700_2021-12-09T235110-0700.parquet.gzip\n",
      "2021-09-13 13:16:46-07:00\n",
      "saving rows 6242747 to 7134567\n",
      "confidence     2\n",
      "value         91\n",
      "Name: 2021-12-09 23:51:15-07:00, dtype: object\n",
      "to a file named 2021-12-09T235115-0700_2022-03-13T154056-0700.parquet.gzip\n",
      "2021-12-09 23:51:15-07:00\n",
      "saving rows 7134568 to 8026388\n",
      "confidence     1\n",
      "value         89\n",
      "Name: 2022-03-13 15:41:11-07:00, dtype: object\n",
      "to a file named 2022-03-13T154111-0700_2022-06-11T141703-0700.parquet.gzip\n",
      "2022-03-13 15:41:11-07:00\n",
      "saving rows 8026389 to 8918209\n",
      "confidence      1\n",
      "value         108\n",
      "Name: 2022-06-11 14:17:08-07:00, dtype: object\n",
      "to a file named 2022-06-11T141708-0700_2022-09-15T223056-0700.parquet.gzip\n",
      "2022-06-11 14:17:08-07:00\n",
      "saving rows 8918210 to 9810030\n",
      "confidence     2\n",
      "value         84\n",
      "Name: 2022-09-15 22:31:01-07:00, dtype: object\n",
      "to a file named 2022-09-15T223101-0700_2022-12-17T094408-0700.parquet.gzip\n",
      "2022-09-15 22:31:01-07:00\n",
      "saving rows 9810031 to 10701851\n",
      "confidence     3\n",
      "value         60\n",
      "Name: 2022-12-17 09:44:13-07:00, dtype: object\n",
      "to a file named 2022-12-17T094413-0700_2023-03-14T014506-0700.parquet.gzip\n",
      "2022-12-17 09:44:13-07:00\n",
      "saving rows 10701852 to 11593672\n",
      "confidence     3\n",
      "value         57\n",
      "Name: 2023-03-14 01:45:11-07:00, dtype: object\n",
      "to a file named 2023-03-14T014511-0700_2023-06-27T174105-0700.parquet.gzip\n",
      "2023-03-14 01:45:11-07:00\n",
      "saving rows 11593673 to 12485493\n",
      "confidence     2\n",
      "value         70\n",
      "Name: 2023-06-27 17:41:08-07:00, dtype: object\n",
      "to a file named 2023-06-27T174108-0700_2023-10-12T024130-0700.parquet.gzip\n",
      "2023-06-27 17:41:08-07:00\n",
      "saving rows 12485494 to 13377314\n",
      "confidence     3\n",
      "value         52\n",
      "Name: 2023-10-12 02:41:35-07:00, dtype: object\n",
      "to a file named 2023-10-12T024135-0700_2024-01-12T193642-0700.parquet.gzip\n",
      "2023-10-12 02:41:35-07:00\n",
      "saving rows 13377315 to 14269135\n",
      "confidence     1\n",
      "value         74\n",
      "Name: 2024-01-12 19:36:57-07:00, dtype: object\n",
      "to a file named 2024-01-12T193657-0700_2024-04-10T073550-0700.parquet.gzip\n",
      "2024-01-12 19:36:57-07:00\n",
      "saving rows 14269136 to 15160956\n",
      "confidence     3\n",
      "value         64\n",
      "Name: 2024-04-10 07:35:55-07:00, dtype: object\n",
      "to a file named 2024-04-10T073555-0700_2024-06-21T013658-0700.parquet.gzip\n",
      "2024-04-10 07:35:55-07:00\n",
      "saving rows 15160957 to 16052777\n",
      "confidence     3\n",
      "value         52\n",
      "Name: 2024-06-21 01:37:03-07:00, dtype: object\n",
      "to a file named 2024-06-21T013703-0700_2024-09-17T125135-0700.parquet.gzip\n",
      "2024-06-21 01:37:03-07:00\n",
      "saving rows 16052778 to 16052784\n",
      "confidence     2\n",
      "value         62\n",
      "Name: 2024-09-17 12:51:45-07:00, dtype: object\n",
      "to a file named 2024-09-17T125145-0700_2024-09-17T125220-0700.parquet.gzip\n",
      "2024-09-17 12:51:45-07:00\n"
     ]
    }
   ],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the size of the file in bytes\n",
    "# workingDataCompleteFile = \"/home/chowder/Documents/workingData/fitbit/hr/fitbitHRdf.parquet.gzip\"\n",
    "# fitbitHRdf.to_parquet(workingDataCompleteFile,\n",
    "#               compression='gzip') \n",
    "\n",
    "# file_size = os.path.getsize(workingDataCompleteFile)\n",
    "# os.remove(workingDataCompleteFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# numFiles = math.ceil(file_size / (1024 * 1024 * 5))\n",
    "# rows_per_file = int(len(fitbitHRdf)/numFiles)\n",
    "\n",
    "# print(f\"the file size of all the data is about {file_size // (1024 * 1024)} MB\")\n",
    "# print(f\"the total number of rows in the file is {len(fitbitHRdf)}\")\n",
    "# print(f\"splitting into {numFiles} number of files with {rows_per_file} rows per file\")\n",
    "\n",
    "# for fileNumber in range(numFiles + 1):\n",
    "#     startRow = fileNumber * rows_per_file\n",
    "#     if fileNumber == numFiles:\n",
    "#         endRow = len(fitbitHRdf) - 1\n",
    "#     else:\n",
    "#         endRow = ((fileNumber + 1) * rows_per_file) - 1\n",
    "\n",
    "#     print(f\"saving rows {startRow} to {endRow}\")\n",
    "#     print(fitbitHRdf.iloc[startRow])\n",
    "\n",
    "#     parquetName = fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \"_\" +\\\n",
    "#                   fitbitHRdf.iloc[endRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "#                   \".parquet.gzip\"\n",
    "#     print(f\"to a file named {parquetName}\")\n",
    "\n",
    "#     print(pd.to_datetime(fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z')))\n",
    "\n",
    "#     fitbitHRdf.iloc[startRow:endRow+1].to_parquet(fitbitHRWorkingDataPath + parquetName,\n",
    "#               compression='gzip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
