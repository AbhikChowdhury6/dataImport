{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the export go to the fitbit app, click on your icon, and order a google takeout \n",
    "# took like 8 hours the last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pathOfExport' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get the list of all files and directories\u001b[39;00m\n\u001b[1;32m     17\u001b[0m exportFilePath \u001b[38;5;241m=\u001b[39m exportsDataPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m27-8-24/takeout-20240828T033834Z-001/Takeout/Fitbit/Global Export Data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m dir_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mpathOfExport\u001b[49m \u001b[38;5;241m+\u001b[39m exportFilePath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pathOfExport' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")+1]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath)\n",
    "from utils import exportsDataPath, workingDataPath, writeHypnoDfParquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fitbitHRWorkingDataPath = workingDataPath + 'fitbit/hr/'\n",
    "\n",
    "# Get the list of all files and directories\n",
    "exportFilePath = exportsDataPath + \"27-8-24/takeout-20240828T033834Z-001/Takeout/Fitbit/Global Export Data/\"\n",
    "dir_list = os.listdir(exportFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 0 rows from 0 files\n"
     ]
    }
   ],
   "source": [
    "# read in the existing files and make the existing df\n",
    "workingDataFiles = os.listdir(fitbitHRWorkingDataPath)\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "dfSoFar = pd.DataFrame(columns=columnNames).set_index(\"sampleDT\")\n",
    "\n",
    "for dataFileName in workingDataFiles:\n",
    "    dfSoFar = pd.concat([dfSoFar, pd.read_parquet(fitbitHRWorkingDataPath + dataFileName)]) \n",
    "\n",
    "dfSoFar = dfSoFar[~dfSoFar.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "print(f\"read in {len(dfSoFar)} rows from {len(workingDataFiles)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of unique dates in the index\n",
    "#   removing the latest 3 days since they might be incomplete\n",
    "if len(dfSoFar) > 0:\n",
    "    datesSoFar = sorted(list(set(dfSoFar.index.date)))[:-3]\n",
    "    print(datesSoFar[-1])\n",
    "    hrFilenames = [x for x in dir_list if x.split(\"_\")[0] == \"heart\"]\n",
    "    hrFilenames = [x for x in hrFilenames if pd.to_datetime(x[11:-5]).date() not in datesSoFar]\n",
    "    hrFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 100000 samples so far\n",
      "added 200000 samples so far\n",
      "added 300000 samples so far\n",
      "added 400000 samples so far\n",
      "added 500000 samples so far\n",
      "added 600000 samples so far\n",
      "added 700000 samples so far\n",
      "added 800000 samples so far\n",
      "added 900000 samples so far\n",
      "added 1000000 samples so far\n",
      "added 1100000 samples so far\n",
      "added 1200000 samples so far\n",
      "added 1300000 samples so far\n",
      "added 1400000 samples so far\n",
      "added 1500000 samples so far\n",
      "added 1600000 samples so far\n",
      "added 1700000 samples so far\n",
      "added 1800000 samples so far\n",
      "added 1900000 samples so far\n",
      "added 2000000 samples so far\n",
      "added 2100000 samples so far\n",
      "added 2200000 samples so far\n",
      "added 2300000 samples so far\n",
      "added 2400000 samples so far\n",
      "added 2500000 samples so far\n",
      "added 2600000 samples so far\n",
      "added 2700000 samples so far\n",
      "added 2800000 samples so far\n",
      "added 2900000 samples so far\n",
      "added 3000000 samples so far\n",
      "added 3100000 samples so far\n",
      "added 3200000 samples so far\n",
      "added 3300000 samples so far\n",
      "added 3400000 samples so far\n",
      "added 3500000 samples so far\n",
      "added 3600000 samples so far\n",
      "added 3700000 samples so far\n",
      "added 3800000 samples so far\n",
      "added 3900000 samples so far\n",
      "added 4000000 samples so far\n",
      "added 4100000 samples so far\n",
      "added 4200000 samples so far\n",
      "added 4300000 samples so far\n",
      "added 4400000 samples so far\n",
      "added 4500000 samples so far\n",
      "added 4600000 samples so far\n",
      "added 4700000 samples so far\n",
      "added 4800000 samples so far\n",
      "added 4900000 samples so far\n",
      "added 5000000 samples so far\n",
      "added 5100000 samples so far\n",
      "added 5200000 samples so far\n",
      "added 5300000 samples so far\n",
      "added 5400000 samples so far\n",
      "added 5500000 samples so far\n",
      "added 5600000 samples so far\n",
      "added 5700000 samples so far\n",
      "added 5800000 samples so far\n",
      "added 5900000 samples so far\n",
      "added 6000000 samples so far\n",
      "added 6100000 samples so far\n",
      "added 6200000 samples so far\n",
      "added 6300000 samples so far\n",
      "added 6400000 samples so far\n",
      "added 6500000 samples so far\n",
      "added 6600000 samples so far\n",
      "added 6700000 samples so far\n",
      "added 6800000 samples so far\n",
      "added 6900000 samples so far\n",
      "added 7000000 samples so far\n",
      "added 7100000 samples so far\n",
      "added 7200000 samples so far\n",
      "added 7300000 samples so far\n",
      "added 7400000 samples so far\n",
      "added 7500000 samples so far\n",
      "added 7600000 samples so far\n",
      "added 7700000 samples so far\n",
      "added 7800000 samples so far\n",
      "added 7900000 samples so far\n",
      "added 8000000 samples so far\n",
      "added 8100000 samples so far\n",
      "added 8200000 samples so far\n",
      "added 8300000 samples so far\n",
      "added 8400000 samples so far\n",
      "added 8500000 samples so far\n",
      "added 8600000 samples so far\n",
      "added 8700000 samples so far\n",
      "added 8800000 samples so far\n",
      "added 8900000 samples so far\n",
      "added 9000000 samples so far\n",
      "added 9100000 samples so far\n",
      "added 9200000 samples so far\n",
      "added 9300000 samples so far\n",
      "added 9400000 samples so far\n",
      "added 9500000 samples so far\n",
      "added 9600000 samples so far\n",
      "added 9700000 samples so far\n",
      "added 9800000 samples so far\n",
      "added 9900000 samples so far\n",
      "added 10000000 samples so far\n",
      "added 10100000 samples so far\n",
      "added 10200000 samples so far\n",
      "added 10300000 samples so far\n",
      "added 10400000 samples so far\n",
      "added 10500000 samples so far\n",
      "added 10600000 samples so far\n",
      "added 10700000 samples so far\n",
      "added 10800000 samples so far\n",
      "added 10900000 samples so far\n",
      "added 11000000 samples so far\n",
      "added 11100000 samples so far\n",
      "added 11200000 samples so far\n",
      "added 11300000 samples so far\n",
      "added 11400000 samples so far\n",
      "added 11500000 samples so far\n",
      "added 11600000 samples so far\n",
      "added 11700000 samples so far\n",
      "added 11800000 samples so far\n",
      "added 11900000 samples so far\n",
      "added 12000000 samples so far\n",
      "added 12100000 samples so far\n",
      "added 12200000 samples so far\n",
      "added 12300000 samples so far\n",
      "added 12400000 samples so far\n",
      "added 12500000 samples so far\n",
      "added 12600000 samples so far\n",
      "added 12700000 samples so far\n",
      "added 12800000 samples so far\n",
      "added 12900000 samples so far\n",
      "added 13000000 samples so far\n",
      "added 13100000 samples so far\n",
      "added 13200000 samples so far\n",
      "added 13300000 samples so far\n",
      "added 13400000 samples so far\n",
      "added 13500000 samples so far\n",
      "added 13600000 samples so far\n",
      "added 13700000 samples so far\n",
      "added 13800000 samples so far\n",
      "added 13900000 samples so far\n",
      "added 14000000 samples so far\n",
      "added 14100000 samples so far\n",
      "added 14200000 samples so far\n",
      "added 14300000 samples so far\n",
      "added 14400000 samples so far\n",
      "added 14500000 samples so far\n",
      "added 14600000 samples so far\n",
      "added 14700000 samples so far\n",
      "added 14800000 samples so far\n",
      "added 14900000 samples so far\n",
      "added 15000000 samples so far\n",
      "added 15100000 samples so far\n",
      "added 15200000 samples so far\n",
      "added 15300000 samples so far\n",
      "added 15400000 samples so far\n",
      "added 15500000 samples so far\n",
      "added 15600000 samples so far\n",
      "added 15700000 samples so far\n",
      "added 15800000 samples so far\n",
      "added 15826367 samples\n"
     ]
    }
   ],
   "source": [
    "#takes like 40 minutes to make 15.8 million rows\n",
    "import json\n",
    "\n",
    "columnNames = [\"sampleDT\", \"confidence\", \"value\"]\n",
    "\n",
    "samplesCount = 0\n",
    "samplesList = []\n",
    "\n",
    "for fn in hrFilenames:\n",
    "    data = json.load(open(exportFilePath + fn))\n",
    "    for row in data:\n",
    "        samplesList.append([pd.to_datetime(row[\"dateTime\"] + \"+0000\").tz_convert(\"US/Arizona\"), row[\"value\"][\"confidence\"], row[\"value\"][\"bpm\"]])\n",
    "        samplesCount += 1\n",
    "        if samplesCount % 100_000 == 0:\n",
    "            print(f\"added {samplesCount} samples so far\")\n",
    "print(f\"added {samplesCount} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesList = sorted(samplesList, key=lambda x: x[0]) #sort by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence    uint8\n",
       "value         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fitbitHRdf = pd.DataFrame(data=samplesList, columns=columnNames)\n",
    "fitbitHRdf = fitbitHRdf.set_index(\"sampleDT\")\n",
    "fitbitHRdf[\"confidence\"] = fitbitHRdf[\"confidence\"].astype('uint8')\n",
    "fitbitHRdf[\"value\"] = fitbitHRdf[\"value\"].astype('uint8')\n",
    "fitbitHRdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the df's\n",
    "# remove duplicate indexes\n",
    "fitbitHRdf = pd.concat([dfSoFar, fitbitHRdf])\n",
    "fitbitHRdf = fitbitHRdf[~fitbitHRdf.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeWorkingHRDfParquet('fitbit', fitbitHRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the file in bytes\n",
    "workingDataCompleteFile = \"/home/chowder/Documents/workingData/fitbit/hr/fitbitHRdf.parquet.gzip\"\n",
    "fitbitHRdf.to_parquet(workingDataCompleteFile,\n",
    "              compression='gzip') \n",
    "\n",
    "file_size = os.path.getsize(workingDataCompleteFile)\n",
    "os.remove(workingDataCompleteFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file size of all the data is about 86 MB\n",
      "the total number of rows in the file is 15826297\n",
      "splitting into 18 number of files with 879238 rows per file\n",
      "\n",
      "saving rows 0 to 879237\n",
      "confidence     1\n",
      "value         70\n",
      "Name: 2020-05-13 17:02:34-07:00, dtype: object\n",
      "to a file named 2020-05-13T170234-0700_2020-08-02T072756-0700.parquet.gzip\n",
      "2020-05-13 17:02:34-07:00\n",
      "saving rows 879238 to 1758475\n",
      "confidence     3\n",
      "value         67\n",
      "Name: 2020-08-02 07:28:51-07:00, dtype: object\n",
      "to a file named 2020-08-02T072851-0700_2020-10-22T191111-0700.parquet.gzip\n",
      "2020-08-02 07:28:51-07:00\n",
      "saving rows 1758476 to 2637713\n",
      "confidence     2\n",
      "value         87\n",
      "Name: 2020-10-22 19:12:06-07:00, dtype: object\n",
      "to a file named 2020-10-22T191206-0700_2021-01-11T152818-0700.parquet.gzip\n",
      "2020-10-22 19:12:06-07:00\n",
      "saving rows 2637714 to 3516951\n",
      "confidence     3\n",
      "value         60\n",
      "Name: 2021-01-11 15:28:33-07:00, dtype: object\n",
      "to a file named 2021-01-11T152833-0700_2021-03-31T170238-0700.parquet.gzip\n",
      "2021-01-11 15:28:33-07:00\n",
      "saving rows 3516952 to 4396189\n",
      "confidence     2\n",
      "value         81\n",
      "Name: 2021-03-31 17:03:13-07:00, dtype: object\n",
      "to a file named 2021-03-31T170313-0700_2021-06-18T160646-0700.parquet.gzip\n",
      "2021-03-31 17:03:13-07:00\n",
      "saving rows 4396190 to 5275427\n",
      "confidence     2\n",
      "value         90\n",
      "Name: 2021-06-18 16:07:22-07:00, dtype: object\n",
      "to a file named 2021-06-18T160722-0700_2021-09-06T215109-0700.parquet.gzip\n",
      "2021-06-18 16:07:22-07:00\n",
      "saving rows 5275428 to 6154665\n",
      "confidence     2\n",
      "value         87\n",
      "Name: 2021-09-06 21:51:44-07:00, dtype: object\n",
      "to a file named 2021-09-06T215144-0700_2021-11-30T203936-0700.parquet.gzip\n",
      "2021-09-06 21:51:44-07:00\n",
      "saving rows 6154666 to 7033903\n",
      "confidence     2\n",
      "value         85\n",
      "Name: 2021-11-30 20:40:06-07:00, dtype: object\n",
      "to a file named 2021-11-30T204006-0700_2022-03-04T162548-0700.parquet.gzip\n",
      "2021-11-30 20:40:06-07:00\n",
      "saving rows 7033904 to 7913141\n",
      "confidence     3\n",
      "value         66\n",
      "Name: 2022-03-04 16:26:03-07:00, dtype: object\n",
      "to a file named 2022-03-04T162603-0700_2022-06-01T092815-0700.parquet.gzip\n",
      "2022-03-04 16:26:03-07:00\n",
      "saving rows 7913142 to 8792379\n",
      "confidence     3\n",
      "value         56\n",
      "Name: 2022-06-01 09:28:45-07:00, dtype: object\n",
      "to a file named 2022-06-01T092845-0700_2022-09-06T154109-0700.parquet.gzip\n",
      "2022-06-01 09:28:45-07:00\n",
      "saving rows 8792380 to 9671617\n",
      "confidence     3\n",
      "value         64\n",
      "Name: 2022-09-06 15:41:39-07:00, dtype: object\n",
      "to a file named 2022-09-06T154139-0700_2022-12-02T212500-0700.parquet.gzip\n",
      "2022-09-06 15:41:39-07:00\n",
      "saving rows 9671618 to 10550855\n",
      "confidence     2\n",
      "value         76\n",
      "Name: 2022-12-02 21:25:12-07:00, dtype: object\n",
      "to a file named 2022-12-02T212512-0700_2023-02-27T110114-0700.parquet.gzip\n",
      "2022-12-02 21:25:12-07:00\n",
      "saving rows 10550856 to 11430093\n",
      "confidence     2\n",
      "value         59\n",
      "Name: 2023-02-27 11:01:49-07:00, dtype: object\n",
      "to a file named 2023-02-27T110149-0700_2023-06-08T213800-0700.parquet.gzip\n",
      "2023-02-27 11:01:49-07:00\n",
      "saving rows 11430094 to 12309331\n",
      "confidence     2\n",
      "value         72\n",
      "Name: 2023-06-08 21:38:25-07:00, dtype: object\n",
      "to a file named 2023-06-08T213825-0700_2023-09-01T201022-0700.parquet.gzip\n",
      "2023-06-08 21:38:25-07:00\n",
      "saving rows 12309332 to 13188569\n",
      "confidence     2\n",
      "value         86\n",
      "Name: 2023-09-01 20:10:37-07:00, dtype: object\n",
      "to a file named 2023-09-01T201037-0700_2023-12-26T162551-0700.parquet.gzip\n",
      "2023-09-01 20:10:37-07:00\n",
      "saving rows 13188570 to 14067807\n",
      "confidence     1\n",
      "value         85\n",
      "Name: 2023-12-26 16:26:31-07:00, dtype: object\n",
      "to a file named 2023-12-26T162631-0700_2024-03-21T191922-0700.parquet.gzip\n",
      "2023-12-26 16:26:31-07:00\n",
      "saving rows 14067808 to 14947045\n",
      "confidence     2\n",
      "value         73\n",
      "Name: 2024-03-21 19:20:02-07:00, dtype: object\n",
      "to a file named 2024-03-21T192002-0700_2024-06-05T024942-0700.parquet.gzip\n",
      "2024-03-21 19:20:02-07:00\n",
      "saving rows 14947046 to 15826283\n",
      "confidence     3\n",
      "value         72\n",
      "Name: 2024-06-05 02:50:02-07:00, dtype: object\n",
      "to a file named 2024-06-05T025002-0700_2024-08-28T065815-0700.parquet.gzip\n",
      "2024-06-05 02:50:02-07:00\n",
      "saving rows 15826284 to 15826296\n",
      "confidence     3\n",
      "value         56\n",
      "Name: 2024-08-28 06:58:20-07:00, dtype: object\n",
      "to a file named 2024-08-28T065820-0700_2024-08-28T065950-0700.parquet.gzip\n",
      "2024-08-28 06:58:20-07:00\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "numFiles = math.ceil(file_size / (1024 * 1024 * 5))\n",
    "rows_per_file = int(len(fitbitHRdf)/numFiles)\n",
    "\n",
    "print(f\"the file size of all the data is about {file_size // (1024 * 1024)} MB\")\n",
    "print(f\"the total number of rows in the file is {len(fitbitHRdf)}\")\n",
    "print(f\"splitting into {numFiles} number of files with {rows_per_file} rows per file\")\n",
    "\n",
    "for fileNumber in range(numFiles + 1):\n",
    "    startRow = fileNumber * rows_per_file\n",
    "    if fileNumber == numFiles:\n",
    "        endRow = len(fitbitHRdf) - 1\n",
    "    else:\n",
    "        endRow = ((fileNumber + 1) * rows_per_file) - 1\n",
    "\n",
    "    print(f\"saving rows {startRow} to {endRow}\")\n",
    "    print(fitbitHRdf.iloc[startRow])\n",
    "\n",
    "    parquetName = fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                  \"_\" +\\\n",
    "                  fitbitHRdf.iloc[endRow].name.strftime('%Y-%m-%dT%H%M%S%z') +\\\n",
    "                  \".parquet.gzip\"\n",
    "    print(f\"to a file named {parquetName}\")\n",
    "\n",
    "    print(pd.to_datetime(fitbitHRdf.iloc[startRow].name.strftime('%Y-%m-%dT%H%M%S%z')))\n",
    "\n",
    "    fitbitHRdf.iloc[startRow:endRow+1].to_parquet(workingDataPath + parquetName,\n",
    "              compression='gzip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
