{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a read and write for the bulk data\n",
    "# there will be different read and write functions for different data types\n",
    "\n",
    "# in the end data will be read in based on the schema of the export file\n",
    "    # but then put into the bulk data schema and with the normal schema\n",
    "\n",
    "# writing 5 minute data files to the right location will be a part of it\n",
    "# splitting the input data into the 5 minute segments will also be a part of it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for the recent captures what would I do\n",
    "# I'll have a dictionary of camera names and metadata for the bulk data location\n",
    "\n",
    "# I'll go through a sorted list of the files\n",
    "# I'll grab the device name from the folder name\n",
    "# I'll go into the folder and combine all the parquets and write < ez jk I need these for the quereies\n",
    "# for the bulk data, read it one file at a time and query the data for each 5 minute segment of the day\n",
    "# this can use the same query function that could be used for querying the bulk data\n",
    "\n",
    "# it will take in a bounds of times and retrun the frames between those times\n",
    "# then a different function will take those frames and write them\n",
    "\n",
    "# it does involve some way of linking the data to the time associated with the data\n",
    "# let's just start with the video data I have \n",
    "\n",
    "# for every 5 minute segment\n",
    "# search the deduplicated file names for the relevant files\n",
    "# read the parquet and line up the times with the frame counts\n",
    "\n",
    "# the metadata should have the time, but ngl it could only have the time\n",
    "# we could get the frame offset by querieng the times for the 5 minute segment\n",
    "# and then lining up the indexes\n",
    "\n",
    "# in that case we can just write all the files and hope they line up\n",
    "# although taking it file by file might help keep errors from propigating\n",
    "# if I had good tests I could trust this would be less of an issue\n",
    "# acctually if the timestamps line up there's no issues\n",
    "# this is only a problem for the old capture data beofre I fixed that timestamp bug\n",
    "# I will still write it file by file to make it so I only have to write one \n",
    "\n",
    "# this schema does mean that the files themselves will be of gratly varying length due to the timestamps\n",
    "# as little as 20 frames per video in timelapse mode which would result in low compression\n",
    "# this is why we had the heterogenious timesamp structure anyway\n",
    "\n",
    "# the essence then could be the same thing, but we treat the file names as the indcators for where the data is\n",
    "# then we query the timestampsdf for that videos frames and calculate offsets for the desired times\n",
    "# return the frames that are \n",
    "\n",
    "# so the takeaway from this is that we can just cut off by day (which will still take some code for audio)\n",
    "# but for the camera structure we don't need to do anything since it's already divided that way.\n",
    "\n",
    "# so back to the standard way I was doing it where I just transfer all the videos as they are to the new day folder\n",
    "# I can also just add all the timestamps to the metadata raw and just make a check incase there are too many timestamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath + \"/dataImport/\")\n",
    "import rwWorkingTSDf\n",
    "from rwWorkingTSDf import writeWorkingTSDf, readWorkingTSDF\n",
    "recentCapPath = repoPath + \"/recentCaptures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries\n",
    "from collections import OrderedDict\n",
    "fieldNames = [\"responsiblePartyName\", \"instanceName\", \"developingPartyName\", \"deviceName\", \"dataType\", \"dataSource\"]\n",
    "myCams = {}\n",
    "myCams[\"mobilepi\"] = [\"abhik\", \"mobilepi\", \"abhik\", \"piCam-raspberryPi5-Camv3120noir\", \"mp4\", \"piVidCap\"]\n",
    "myCams[\"bedroompi\"] = [\"abhik\", \"bedroompi\", \"abhik\", \"piCam-raspberryPi5-Camv3120\", \"mp4\", \"piVidCap\"]\n",
    "myCams[\"bathroompi\"] = [\"abhik\", \"bathroompi\", \"abhik\", \"piCam-raspberryPi5-Camv3noir\", \"mp4\", \"piVidCap\"]\n",
    "myCams[\"bathroomCam\"] = [\"abhik\", \"bathroomCam\", \"abhik\", \"pcCam-webcam\", \"mp4\", \"vidCap\"]\n",
    "myCams[\"deskCam\"] = [\"abhik\", \"deskCam\", \"abhik\", \"pcCam-webcam\", \"mp4\", \"vidCap\"]\n",
    "# right now we are going through and writing the intake code for recent video captures\n",
    "# later I would like to refactor the pividcap and the normal vidcap\n",
    "# and clear out everything un needed for video capture in the repo\n",
    "# I honestly should be writing this in data import\n",
    "\n",
    "\n",
    "# damn I have a bunch of files seperated by the current timezone day and not the UTC day\n",
    "# this does mean that there will be some cutting together and apart of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the metadata parquet\n",
    "workingdfs = {}\n",
    "for k in myCams.keys():\n",
    "    workingdfs[k] = readWorkingTSDF(*myCams[k])\n",
    "\n",
    "\n",
    "foldersToImport = os.listdir(recentCapPath)\n",
    "for f in foldersToImport:\n",
    "    camName = f.split(\"_\")[0]\n",
    "    # get a list of file names\n",
    "    fileNames = set([x.split(\".\")[0] for x in os.listdir(recentCapPath + f)])\n",
    "    # combine all parquets\n",
    "    for i, fileName in enumerate(fileNames):\n",
    "        source = recentCapPath + f + \"/\" + fileName + \".parquet\"\n",
    "        if i == 0:\n",
    "            readDf = pd.read_parquet(source)\n",
    "        else:\n",
    "            readDf = pd.concat(readDf, pd.read_parquet(source))\n",
    "\n",
    "    # add to the full metadata parquet\n",
    "    workingdfs[camName] = pd.concat(workingdfs[camName], readDf).sort_index()\n",
    "\n",
    "    # for each file in a folder\n",
    "        # if not a boundary file move and contuinue\n",
    "        \n",
    "        # the file contains more than one day of data\n",
    "        # calc a list of days the file covers\n",
    "        # read the video file into memory\n",
    "        # for every day\n",
    "            # define the new name\n",
    "            # query the metadata parquet for the frame indexes for that day\n",
    "            # write that slice of the video data to the new location\n",
    "    \n",
    "    # delete the folder\n",
    "\n",
    "    pass\n",
    "\n",
    "# we'll want a function that takes in a file name\n",
    "# calculates all the days it covers\n",
    "# iterates over each of the bounds for those days\n",
    "    # queries the indexes from the data frame\n",
    "    # saves those indexes to a new file\n",
    "\n",
    "# honestly this can be folded into the normal moving processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
