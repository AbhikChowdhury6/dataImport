{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhik\\Documents\\GitHub\\dataImport\\utils.py:21: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + dataFileName)])\n",
      "c:\\Users\\abhik\\Documents\\GitHub\\dataImport\\utils.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + dataFileName)])\n",
      "c:\\Users\\abhik\\Documents\\GitHub\\dataImport\\utils.py:21: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + dataFileName)])\n",
      "c:\\Users\\abhik\\Documents\\GitHub\\dataImport\\utils.py:21: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  dfSoFar = pd.concat([dfSoFar, pd.read_parquet(workingDataHRPath + dataFileName)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "def getRepoPath():\n",
    "    cwd = os.getcwd()\n",
    "    delimiter = \"\\\\\" if \"\\\\\" in cwd else \"/\"\n",
    "    repoPath = delimiter.join(cwd.split(delimiter)[:cwd.split(delimiter).index(\"dataImport\")]) + delimiter\n",
    "    return repoPath\n",
    "repoPath = getRepoPath()\n",
    "sys.path.append(repoPath + 'dataImport/')\n",
    "from utils import workingDataPath, getWorkingHRDfParquet\n",
    "import pandas as pd\n",
    "\n",
    "appleHRDf = getWorkingHRDfParquet('apple')\n",
    "polarHRDf = getWorkingHRDfParquet('polar')\n",
    "fitbitHRDf = getWorkingHRDfParquet('fitbit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersections kinda look like this\n",
    "#    |-|   |---|   |-|   |-|      |---|   |--------|\n",
    "# |-|    |---|    |---|      |-|    |---|  |-| |-|\n",
    "testGroups1 = [[40, 50], [70, 90], [110, 120], [140, 150], [180, 200], [220, 280]]\n",
    "testGroups2 = [[20, 30], [60, 80], [100, 130], [160, 170], [190, 210], [230, 240], [250,270]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of [[startTimestamp, endTimestamp], ...] \n",
    "# for every group with samples always less than maxDelta apart\n",
    "# and at least coveres minGroupTime\n",
    "def getGroups(series, maxDelta = 20, minGroupTime = pd.Timedelta(minutes=5)):\n",
    "    # add a column that is the time to next reading\n",
    "    timesSeries = pd.Series(series.index)\n",
    "    betweenMesures = ((timesSeries.shift(-1) - timesSeries)).dt.total_seconds()\n",
    "    timeToNextDF = pd.DataFrame(timesSeries)\n",
    "    timeToNextDF[\"timeToNextReading\"]  =  betweenMesures\n",
    "    # print(timeToNextDF.head(2))\n",
    "\n",
    "    #filter out time to next reading that are too high\n",
    "    ttnrGreaterThanThresh = timeToNextDF[timeToNextDF[\"timeToNextReading\"] < maxDelta]\n",
    "    # print(ttnrGreaterThanThresh.head(2))\n",
    "\n",
    "    #the indexes that have been removed indicate the boundaries of contiguious sections\n",
    "    index_diff = ttnrGreaterThanThresh.index.to_series().diff()\n",
    "    boundaryIndicators = index_diff[index_diff > 1].index.to_list()\n",
    "    # print(boundaryIndicators)\n",
    "\n",
    "    #add each bounded area to groupBoundaries, if it lasts longer than the minGroup time\n",
    "    clusterStartIndex = 0\n",
    "    groupBoundaries = []\n",
    "    for bii in range(len(boundaryIndicators)): #bii being boundaryIndicatorIndex\n",
    "        startTime = timeToNextDF[\"sampleDT\"].iloc[clusterStartIndex]\n",
    "        endTime = timeToNextDF[\"sampleDT\"].iloc[boundaryIndicators[bii] - 1]\n",
    "        # print( endTime - startTime)\n",
    "        if endTime - startTime >= minGroupTime:\n",
    "            groupBoundaries.append([startTime, endTime])\n",
    "        \n",
    "        clusterStartIndex = boundaryIndicators[bii]\n",
    "\n",
    "    \n",
    "    return groupBoundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "polarGroups = getGroups(polarHRDf, 5)\n",
    "print(len(polarGroups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          value\n",
      "sampleDT                       \n",
      "2019-10-27 15:55:28-07:00    69\n",
      "2019-10-27 15:55:29-07:00    67\n",
      "2019-10-27 15:55:30-07:00    66\n",
      "2019-10-27 16:16:57-07:00    95\n",
      "2019-10-27 16:16:58-07:00    97\n",
      "                          value\n",
      "sampleDT                       \n",
      "2019-10-27 16:35:54-07:00    86\n",
      "2019-10-27 16:35:55-07:00    86\n",
      "2019-10-28 18:26:07-07:00    85\n",
      "2019-10-28 18:26:08-07:00    82\n",
      "2019-10-28 18:26:09-07:00    80\n"
     ]
    }
   ],
   "source": [
    "print(polarHRDf.iloc[160:165])\n",
    "print(polarHRDf.iloc[1300:1305])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2019-10-28 18:26:07-0700', tz='pytz.FixedOffset(-420)'), Timestamp('2019-10-28 18:51:46-0700', tz='pytz.FixedOffset(-420)')]\n",
      "0 days 00:25:39\n",
      "1538\n",
      "[Timestamp('2019-10-28 19:25:14-0700', tz='pytz.FixedOffset(-420)'), Timestamp('2019-10-28 19:31:59-0700', tz='pytz.FixedOffset(-420)')]\n",
      "0 days 00:06:45\n",
      "404\n",
      "[Timestamp('2019-10-28 19:35:43-0700', tz='pytz.FixedOffset(-420)'), Timestamp('2019-10-28 23:59:06-0700', tz='pytz.FixedOffset(-420)')]\n",
      "0 days 04:23:23\n",
      "15802\n",
      "[Timestamp('2019-10-29 00:16:21-0700', tz='pytz.FixedOffset(-420)'), Timestamp('2019-10-29 07:35:05-0700', tz='pytz.FixedOffset(-420)')]\n",
      "0 days 07:18:44\n",
      "26323\n"
     ]
    }
   ],
   "source": [
    "def printHRDfGroupsInfo(HRDf, groups, index):\n",
    "    groupVals = HRDf[(HRDf.index > groups[index][0]) & (HRDf.index < groups[index][1])]\n",
    "    print(groups[index][1] - groups[index][0])\n",
    "    print(len(groupVals))\n",
    "\n",
    "print(polarGroups[1])\n",
    "printHRDfGroupsInfo(polarHRDf, polarGroups, 1)\n",
    "print(polarGroups[2])\n",
    "printHRDfGroupsInfo(polarHRDf, polarGroups, 2)\n",
    "\n",
    "print(polarGroups[3])\n",
    "printHRDfGroupsInfo(polarHRDf, polarGroups, 3)\n",
    "\n",
    "print(polarGroups[4])\n",
    "printHRDfGroupsInfo(polarHRDf, polarGroups, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492\n",
      "30\n",
      "24776\n"
     ]
    }
   ],
   "source": [
    "appleGroups = getGroups(appleHRDf, 600)\n",
    "print(len(appleGroups))\n",
    "polarGroups = getGroups(polarHRDf, 5)\n",
    "print(len(polarGroups))\n",
    "fitbitGroups = getGroups(fitbitHRDf, 20)\n",
    "print(len(fitbitGroups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually the grouping function I already wrote looks great\n",
    "# what I want is the intersecting function\n",
    "#it just has to take in two groups and return the intersection\n",
    "# that can be called iteratively for every group\n",
    "\n",
    "def calcIntersection(groups1, groups2):\n",
    "\n",
    "    intersectingGroups = []\n",
    "\n",
    "    groups1i = 0\n",
    "    groups2i = 0\n",
    "\n",
    "    # go though every group in group 1\n",
    "    while groups1i < len(groups1) and groups2i < len(groups2):\n",
    "        # no intersctions \n",
    "        # if the end time of group 2 is less than the start time of group 1\n",
    "        if groups1[groups1i][1] < groups2[groups2i][0]:\n",
    "            #no intersection, go to the next group in groups1\n",
    "            groups1i += 1\n",
    "            continue\n",
    "        \n",
    "        # if the groups1 group we're looking starts after the end of the group 2 group\n",
    "        if groups1[groups1i][0] > groups2[groups2i][1]:\n",
    "            #no intersection, and check the next late group\n",
    "            groups2i += 1\n",
    "            continue\n",
    "        \n",
    "        # print(f\"groups1[{groups1i}]{groups1[groups1i]}\")\n",
    "        # print(f\"groups2[{groups2i}]{groups2[groups2i]}\")\n",
    "\n",
    "        # there is an intersection and it is the latest of the start times\n",
    "            # and the earliest of the end times\n",
    "        intersectionStart = max(groups1[groups1i][0], groups2[groups2i][0])\n",
    "        intersectionEnd = min(groups1[groups1i][1], groups2[groups2i][1])\n",
    "        intersectingGroups.append([intersectionStart, intersectionEnd])\n",
    "\n",
    "        #whichever set had the group with the earliest end time, iterate that one\n",
    "        if groups1[groups1i][1] <= groups2[groups2i][1]:\n",
    "            groups1i += 1\n",
    "        else:\n",
    "            groups2i += 1\n",
    "        \n",
    "    return intersectingGroups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calcIntersection(appleGroups, polarGroups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18756"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calcIntersection(appleGroups, fitbitGroups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
